{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RMLHu3RZ5lWK"
   },
   "source": [
    "# iris Data Keras Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z0vCjo566DU3"
   },
   "source": [
    "* Colab File Upload\n",
    " - iris.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 43,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 33385,
     "status": "ok",
     "timestamp": 1584589157184,
     "user": {
      "displayName": "j gf",
      "photoUrl": "",
      "userId": "13586540763833363239"
     },
     "user_tz": -540
    },
    "id": "V37qMxNI5S44",
    "outputId": "660a95d8-27f4-4d96-ab77-3a9e670c9a4c"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d9ce1d3f1267>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0muploaded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0muploaded\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "for fn in uploaded.keys():\n",
    "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "      name = fn, length = len(uploaded[fn])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_2m1joqE6eEq"
   },
   "source": [
    "* pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1280,
     "status": "ok",
     "timestamp": 1574488306151,
     "user": {
      "displayName": "이정구",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCJOG97rou_jpwsvziQA7tTe2JnwoyotzfHjlO2=s64",
      "userId": "17400242557229223804"
     },
     "user_tz": -540
    },
    "id": "F6MvmhtB53CS",
    "outputId": "5a5e4774-f429-4980-e639-307ee639f6d1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sepal.Length</th>\n",
       "      <th>Sepal.Width</th>\n",
       "      <th>Petal.Length</th>\n",
       "      <th>Petal.Width</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sepal.Length  Sepal.Width  Petal.Length  Petal.Width Species\n",
       "0           5.1          3.5           1.4          0.2  setosa\n",
       "1           4.9          3.0           1.4          0.2  setosa\n",
       "2           4.7          3.2           1.3          0.2  setosa\n",
       "3           4.6          3.1           1.5          0.2  setosa\n",
       "4           5.0          3.6           1.4          0.2  setosa"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "iris = pd.read_csv('C:/Users/moonjong/jupyter_notebook/iris.csv')\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0a9zRCoM6iHg"
   },
   "source": [
    "# I. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WIq-NRIq659b"
   },
   "source": [
    "> ## 1) iris.Species 빈도분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sg3g1ycL0bj_"
   },
   "source": [
    "* Species : setosa, virginica, versicolor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 770,
     "status": "ok",
     "timestamp": 1574488461154,
     "user": {
      "displayName": "이정구",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCJOG97rou_jpwsvziQA7tTe2JnwoyotzfHjlO2=s64",
      "userId": "17400242557229223804"
     },
     "user_tz": -540
    },
    "id": "Ccv6L9LX6mBD",
    "outputId": "e3c2e4b0-8c6b-49a7-ced5-2ffb846fae36"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "setosa        50\n",
       "versicolor    50\n",
       "virginica     50\n",
       "Name: Species, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.Species.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JrKQmY9s7WzT"
   },
   "source": [
    "> ## 2) DataFrame to Array & Casting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R9VIgOnfE4Ra"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2, 'setosa'],\n",
       "       [4.9, 3.0, 1.4, 0.2, 'setosa'],\n",
       "       [4.7, 3.2, 1.3, 0.2, 'setosa'],\n",
       "       [4.6, 3.1, 1.5, 0.2, 'setosa'],\n",
       "       [5.0, 3.6, 1.4, 0.2, 'setosa'],\n",
       "       [5.4, 3.9, 1.7, 0.4, 'setosa'],\n",
       "       [4.6, 3.4, 1.4, 0.3, 'setosa'],\n",
       "       [5.0, 3.4, 1.5, 0.2, 'setosa'],\n",
       "       [4.4, 2.9, 1.4, 0.2, 'setosa'],\n",
       "       [4.9, 3.1, 1.5, 0.1, 'setosa'],\n",
       "       [5.4, 3.7, 1.5, 0.2, 'setosa'],\n",
       "       [4.8, 3.4, 1.6, 0.2, 'setosa'],\n",
       "       [4.8, 3.0, 1.4, 0.1, 'setosa'],\n",
       "       [4.3, 3.0, 1.1, 0.1, 'setosa'],\n",
       "       [5.8, 4.0, 1.2, 0.2, 'setosa'],\n",
       "       [5.7, 4.4, 1.5, 0.4, 'setosa'],\n",
       "       [5.4, 3.9, 1.3, 0.4, 'setosa'],\n",
       "       [5.1, 3.5, 1.4, 0.3, 'setosa'],\n",
       "       [5.7, 3.8, 1.7, 0.3, 'setosa'],\n",
       "       [5.1, 3.8, 1.5, 0.3, 'setosa'],\n",
       "       [5.4, 3.4, 1.7, 0.2, 'setosa'],\n",
       "       [5.1, 3.7, 1.5, 0.4, 'setosa'],\n",
       "       [4.6, 3.6, 1.0, 0.2, 'setosa'],\n",
       "       [5.1, 3.3, 1.7, 0.5, 'setosa'],\n",
       "       [4.8, 3.4, 1.9, 0.2, 'setosa'],\n",
       "       [5.0, 3.0, 1.6, 0.2, 'setosa'],\n",
       "       [5.0, 3.4, 1.6, 0.4, 'setosa'],\n",
       "       [5.2, 3.5, 1.5, 0.2, 'setosa'],\n",
       "       [5.2, 3.4, 1.4, 0.2, 'setosa'],\n",
       "       [4.7, 3.2, 1.6, 0.2, 'setosa'],\n",
       "       [4.8, 3.1, 1.6, 0.2, 'setosa'],\n",
       "       [5.4, 3.4, 1.5, 0.4, 'setosa'],\n",
       "       [5.2, 4.1, 1.5, 0.1, 'setosa'],\n",
       "       [5.5, 4.2, 1.4, 0.2, 'setosa'],\n",
       "       [4.9, 3.1, 1.5, 0.2, 'setosa'],\n",
       "       [5.0, 3.2, 1.2, 0.2, 'setosa'],\n",
       "       [5.5, 3.5, 1.3, 0.2, 'setosa'],\n",
       "       [4.9, 3.6, 1.4, 0.1, 'setosa'],\n",
       "       [4.4, 3.0, 1.3, 0.2, 'setosa'],\n",
       "       [5.1, 3.4, 1.5, 0.2, 'setosa'],\n",
       "       [5.0, 3.5, 1.3, 0.3, 'setosa'],\n",
       "       [4.5, 2.3, 1.3, 0.3, 'setosa'],\n",
       "       [4.4, 3.2, 1.3, 0.2, 'setosa'],\n",
       "       [5.0, 3.5, 1.6, 0.6, 'setosa'],\n",
       "       [5.1, 3.8, 1.9, 0.4, 'setosa'],\n",
       "       [4.8, 3.0, 1.4, 0.3, 'setosa'],\n",
       "       [5.1, 3.8, 1.6, 0.2, 'setosa'],\n",
       "       [4.6, 3.2, 1.4, 0.2, 'setosa'],\n",
       "       [5.3, 3.7, 1.5, 0.2, 'setosa'],\n",
       "       [5.0, 3.3, 1.4, 0.2, 'setosa'],\n",
       "       [7.0, 3.2, 4.7, 1.4, 'versicolor'],\n",
       "       [6.4, 3.2, 4.5, 1.5, 'versicolor'],\n",
       "       [6.9, 3.1, 4.9, 1.5, 'versicolor'],\n",
       "       [5.5, 2.3, 4.0, 1.3, 'versicolor'],\n",
       "       [6.5, 2.8, 4.6, 1.5, 'versicolor'],\n",
       "       [5.7, 2.8, 4.5, 1.3, 'versicolor'],\n",
       "       [6.3, 3.3, 4.7, 1.6, 'versicolor'],\n",
       "       [4.9, 2.4, 3.3, 1.0, 'versicolor'],\n",
       "       [6.6, 2.9, 4.6, 1.3, 'versicolor'],\n",
       "       [5.2, 2.7, 3.9, 1.4, 'versicolor'],\n",
       "       [5.0, 2.0, 3.5, 1.0, 'versicolor'],\n",
       "       [5.9, 3.0, 4.2, 1.5, 'versicolor'],\n",
       "       [6.0, 2.2, 4.0, 1.0, 'versicolor'],\n",
       "       [6.1, 2.9, 4.7, 1.4, 'versicolor'],\n",
       "       [5.6, 2.9, 3.6, 1.3, 'versicolor'],\n",
       "       [6.7, 3.1, 4.4, 1.4, 'versicolor'],\n",
       "       [5.6, 3.0, 4.5, 1.5, 'versicolor'],\n",
       "       [5.8, 2.7, 4.1, 1.0, 'versicolor'],\n",
       "       [6.2, 2.2, 4.5, 1.5, 'versicolor'],\n",
       "       [5.6, 2.5, 3.9, 1.1, 'versicolor'],\n",
       "       [5.9, 3.2, 4.8, 1.8, 'versicolor'],\n",
       "       [6.1, 2.8, 4.0, 1.3, 'versicolor'],\n",
       "       [6.3, 2.5, 4.9, 1.5, 'versicolor'],\n",
       "       [6.1, 2.8, 4.7, 1.2, 'versicolor'],\n",
       "       [6.4, 2.9, 4.3, 1.3, 'versicolor'],\n",
       "       [6.6, 3.0, 4.4, 1.4, 'versicolor'],\n",
       "       [6.8, 2.8, 4.8, 1.4, 'versicolor'],\n",
       "       [6.7, 3.0, 5.0, 1.7, 'versicolor'],\n",
       "       [6.0, 2.9, 4.5, 1.5, 'versicolor'],\n",
       "       [5.7, 2.6, 3.5, 1.0, 'versicolor'],\n",
       "       [5.5, 2.4, 3.8, 1.1, 'versicolor'],\n",
       "       [5.5, 2.4, 3.7, 1.0, 'versicolor'],\n",
       "       [5.8, 2.7, 3.9, 1.2, 'versicolor'],\n",
       "       [6.0, 2.7, 5.1, 1.6, 'versicolor'],\n",
       "       [5.4, 3.0, 4.5, 1.5, 'versicolor'],\n",
       "       [6.0, 3.4, 4.5, 1.6, 'versicolor'],\n",
       "       [6.7, 3.1, 4.7, 1.5, 'versicolor'],\n",
       "       [6.3, 2.3, 4.4, 1.3, 'versicolor'],\n",
       "       [5.6, 3.0, 4.1, 1.3, 'versicolor'],\n",
       "       [5.5, 2.5, 4.0, 1.3, 'versicolor'],\n",
       "       [5.5, 2.6, 4.4, 1.2, 'versicolor'],\n",
       "       [6.1, 3.0, 4.6, 1.4, 'versicolor'],\n",
       "       [5.8, 2.6, 4.0, 1.2, 'versicolor'],\n",
       "       [5.0, 2.3, 3.3, 1.0, 'versicolor'],\n",
       "       [5.6, 2.7, 4.2, 1.3, 'versicolor'],\n",
       "       [5.7, 3.0, 4.2, 1.2, 'versicolor'],\n",
       "       [5.7, 2.9, 4.2, 1.3, 'versicolor'],\n",
       "       [6.2, 2.9, 4.3, 1.3, 'versicolor'],\n",
       "       [5.1, 2.5, 3.0, 1.1, 'versicolor'],\n",
       "       [5.7, 2.8, 4.1, 1.3, 'versicolor'],\n",
       "       [6.3, 3.3, 6.0, 2.5, 'virginica'],\n",
       "       [5.8, 2.7, 5.1, 1.9, 'virginica'],\n",
       "       [7.1, 3.0, 5.9, 2.1, 'virginica'],\n",
       "       [6.3, 2.9, 5.6, 1.8, 'virginica'],\n",
       "       [6.5, 3.0, 5.8, 2.2, 'virginica'],\n",
       "       [7.6, 3.0, 6.6, 2.1, 'virginica'],\n",
       "       [4.9, 2.5, 4.5, 1.7, 'virginica'],\n",
       "       [7.3, 2.9, 6.3, 1.8, 'virginica'],\n",
       "       [6.7, 2.5, 5.8, 1.8, 'virginica'],\n",
       "       [7.2, 3.6, 6.1, 2.5, 'virginica'],\n",
       "       [6.5, 3.2, 5.1, 2.0, 'virginica'],\n",
       "       [6.4, 2.7, 5.3, 1.9, 'virginica'],\n",
       "       [6.8, 3.0, 5.5, 2.1, 'virginica'],\n",
       "       [5.7, 2.5, 5.0, 2.0, 'virginica'],\n",
       "       [5.8, 2.8, 5.1, 2.4, 'virginica'],\n",
       "       [6.4, 3.2, 5.3, 2.3, 'virginica'],\n",
       "       [6.5, 3.0, 5.5, 1.8, 'virginica'],\n",
       "       [7.7, 3.8, 6.7, 2.2, 'virginica'],\n",
       "       [7.7, 2.6, 6.9, 2.3, 'virginica'],\n",
       "       [6.0, 2.2, 5.0, 1.5, 'virginica'],\n",
       "       [6.9, 3.2, 5.7, 2.3, 'virginica'],\n",
       "       [5.6, 2.8, 4.9, 2.0, 'virginica'],\n",
       "       [7.7, 2.8, 6.7, 2.0, 'virginica'],\n",
       "       [6.3, 2.7, 4.9, 1.8, 'virginica'],\n",
       "       [6.7, 3.3, 5.7, 2.1, 'virginica'],\n",
       "       [7.2, 3.2, 6.0, 1.8, 'virginica'],\n",
       "       [6.2, 2.8, 4.8, 1.8, 'virginica'],\n",
       "       [6.1, 3.0, 4.9, 1.8, 'virginica'],\n",
       "       [6.4, 2.8, 5.6, 2.1, 'virginica'],\n",
       "       [7.2, 3.0, 5.8, 1.6, 'virginica'],\n",
       "       [7.4, 2.8, 6.1, 1.9, 'virginica'],\n",
       "       [7.9, 3.8, 6.4, 2.0, 'virginica'],\n",
       "       [6.4, 2.8, 5.6, 2.2, 'virginica'],\n",
       "       [6.3, 2.8, 5.1, 1.5, 'virginica'],\n",
       "       [6.1, 2.6, 5.6, 1.4, 'virginica'],\n",
       "       [7.7, 3.0, 6.1, 2.3, 'virginica'],\n",
       "       [6.3, 3.4, 5.6, 2.4, 'virginica'],\n",
       "       [6.4, 3.1, 5.5, 1.8, 'virginica'],\n",
       "       [6.0, 3.0, 4.8, 1.8, 'virginica'],\n",
       "       [6.9, 3.1, 5.4, 2.1, 'virginica'],\n",
       "       [6.7, 3.1, 5.6, 2.4, 'virginica'],\n",
       "       [6.9, 3.1, 5.1, 2.3, 'virginica'],\n",
       "       [5.8, 2.7, 5.1, 1.9, 'virginica'],\n",
       "       [6.8, 3.2, 5.9, 2.3, 'virginica'],\n",
       "       [6.7, 3.3, 5.7, 2.5, 'virginica'],\n",
       "       [6.7, 3.0, 5.2, 2.3, 'virginica'],\n",
       "       [6.3, 2.5, 5.0, 1.9, 'virginica'],\n",
       "       [6.5, 3.0, 5.2, 2.0, 'virginica'],\n",
       "       [6.2, 3.4, 5.4, 2.3, 'virginica'],\n",
       "       [5.9, 3.0, 5.1, 1.8, 'virginica']], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_TR = iris.values\n",
    "iris_TR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zByopArAICpw"
   },
   "source": [
    "* object to float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 742,
     "status": "ok",
     "timestamp": 1574488470116,
     "user": {
      "displayName": "이정구",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCJOG97rou_jpwsvziQA7tTe2JnwoyotzfHjlO2=s64",
      "userId": "17400242557229223804"
     },
     "user_tz": -540
    },
    "id": "tPJYM6B3Y64I",
    "outputId": "386cf460-832c-49ef-9342-2dee968f933e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150, 4), (150,))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TR_X = iris_TR[:,0:4].astype(float)\n",
    "TR_Y = iris_TR[:,4]\n",
    "\n",
    "TR_X.shape, TR_Y.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uZzMibX6728v"
   },
   "source": [
    "> ## 3) Normalization with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7T1r7cSrIOYK"
   },
   "source": [
    "* normalize( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3HXjA_IjZMp8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.80377277, 0.55160877, 0.22064351, 0.0315205 ],\n",
       "       [0.82813287, 0.50702013, 0.23660939, 0.03380134],\n",
       "       [0.80533308, 0.54831188, 0.2227517 , 0.03426949],\n",
       "       [0.80003025, 0.53915082, 0.26087943, 0.03478392],\n",
       "       [0.790965  , 0.5694948 , 0.2214702 , 0.0316386 ],\n",
       "       [0.78417499, 0.5663486 , 0.2468699 , 0.05808704],\n",
       "       [0.78010936, 0.57660257, 0.23742459, 0.0508767 ],\n",
       "       [0.80218492, 0.54548574, 0.24065548, 0.0320874 ],\n",
       "       [0.80642366, 0.5315065 , 0.25658935, 0.03665562],\n",
       "       [0.81803119, 0.51752994, 0.25041771, 0.01669451],\n",
       "       [0.80373519, 0.55070744, 0.22325977, 0.02976797],\n",
       "       [0.786991  , 0.55745196, 0.26233033, 0.03279129],\n",
       "       [0.82307218, 0.51442011, 0.24006272, 0.01714734],\n",
       "       [0.8025126 , 0.55989251, 0.20529392, 0.01866308],\n",
       "       [0.81120865, 0.55945424, 0.16783627, 0.02797271],\n",
       "       [0.77381111, 0.59732787, 0.2036345 , 0.05430253],\n",
       "       [0.79428944, 0.57365349, 0.19121783, 0.05883625],\n",
       "       [0.80327412, 0.55126656, 0.22050662, 0.04725142],\n",
       "       [0.8068282 , 0.53788547, 0.24063297, 0.04246464],\n",
       "       [0.77964883, 0.58091482, 0.22930848, 0.0458617 ],\n",
       "       [0.8173379 , 0.51462016, 0.25731008, 0.03027177],\n",
       "       [0.78591858, 0.57017622, 0.23115252, 0.06164067],\n",
       "       [0.77577075, 0.60712493, 0.16864581, 0.03372916],\n",
       "       [0.80597792, 0.52151512, 0.26865931, 0.07901744],\n",
       "       [0.776114  , 0.54974742, 0.30721179, 0.03233808],\n",
       "       [0.82647451, 0.4958847 , 0.26447184, 0.03305898],\n",
       "       [0.79778206, 0.5424918 , 0.25529026, 0.06382256],\n",
       "       [0.80641965, 0.54278246, 0.23262105, 0.03101614],\n",
       "       [0.81609427, 0.5336001 , 0.21971769, 0.03138824],\n",
       "       [0.79524064, 0.54144043, 0.27072022, 0.03384003],\n",
       "       [0.80846584, 0.52213419, 0.26948861, 0.03368608],\n",
       "       [0.82225028, 0.51771314, 0.22840286, 0.06090743],\n",
       "       [0.76578311, 0.60379053, 0.22089897, 0.0147266 ],\n",
       "       [0.77867447, 0.59462414, 0.19820805, 0.02831544],\n",
       "       [0.81768942, 0.51731371, 0.25031309, 0.03337508],\n",
       "       [0.82512295, 0.52807869, 0.19802951, 0.03300492],\n",
       "       [0.82699754, 0.52627116, 0.19547215, 0.03007264],\n",
       "       [0.78523221, 0.5769053 , 0.22435206, 0.01602515],\n",
       "       [0.80212413, 0.54690282, 0.23699122, 0.03646019],\n",
       "       [0.80779568, 0.53853046, 0.23758697, 0.03167826],\n",
       "       [0.80033301, 0.56023311, 0.20808658, 0.04801998],\n",
       "       [0.86093857, 0.44003527, 0.24871559, 0.0573959 ],\n",
       "       [0.78609038, 0.57170209, 0.23225397, 0.03573138],\n",
       "       [0.78889479, 0.55222635, 0.25244633, 0.09466737],\n",
       "       [0.76693897, 0.57144472, 0.28572236, 0.06015208],\n",
       "       [0.82210585, 0.51381615, 0.23978087, 0.05138162],\n",
       "       [0.77729093, 0.57915795, 0.24385598, 0.030482  ],\n",
       "       [0.79594782, 0.55370283, 0.24224499, 0.03460643],\n",
       "       [0.79837025, 0.55735281, 0.22595384, 0.03012718],\n",
       "       [0.81228363, 0.5361072 , 0.22743942, 0.03249135],\n",
       "       [0.76701103, 0.35063361, 0.51499312, 0.15340221],\n",
       "       [0.74549757, 0.37274878, 0.52417798, 0.17472599],\n",
       "       [0.75519285, 0.33928954, 0.53629637, 0.16417236],\n",
       "       [0.75384916, 0.31524601, 0.54825394, 0.17818253],\n",
       "       [0.7581754 , 0.32659863, 0.5365549 , 0.17496355],\n",
       "       [0.72232962, 0.35482858, 0.57026022, 0.16474184],\n",
       "       [0.72634846, 0.38046824, 0.54187901, 0.18446945],\n",
       "       [0.75916547, 0.37183615, 0.51127471, 0.15493173],\n",
       "       [0.76301853, 0.33526572, 0.53180079, 0.15029153],\n",
       "       [0.72460233, 0.37623583, 0.54345175, 0.19508524],\n",
       "       [0.76923077, 0.30769231, 0.53846154, 0.15384615],\n",
       "       [0.73923462, 0.37588201, 0.52623481, 0.187941  ],\n",
       "       [0.78892752, 0.28927343, 0.52595168, 0.13148792],\n",
       "       [0.73081412, 0.34743622, 0.56308629, 0.16772783],\n",
       "       [0.75911707, 0.3931142 , 0.48800383, 0.17622361],\n",
       "       [0.76945444, 0.35601624, 0.50531337, 0.16078153],\n",
       "       [0.70631892, 0.37838513, 0.5675777 , 0.18919257],\n",
       "       [0.75676497, 0.35228714, 0.53495455, 0.13047672],\n",
       "       [0.76444238, 0.27125375, 0.55483721, 0.18494574],\n",
       "       [0.76185188, 0.34011245, 0.53057542, 0.14964948],\n",
       "       [0.6985796 , 0.37889063, 0.56833595, 0.21312598],\n",
       "       [0.77011854, 0.35349703, 0.50499576, 0.16412362],\n",
       "       [0.74143307, 0.29421947, 0.57667016, 0.17653168],\n",
       "       [0.73659895, 0.33811099, 0.56754345, 0.14490471],\n",
       "       [0.76741698, 0.34773582, 0.51560829, 0.15588157],\n",
       "       [0.76785726, 0.34902603, 0.51190484, 0.16287881],\n",
       "       [0.76467269, 0.31486523, 0.53976896, 0.15743261],\n",
       "       [0.74088576, 0.33173989, 0.55289982, 0.18798594],\n",
       "       [0.73350949, 0.35452959, 0.55013212, 0.18337737],\n",
       "       [0.78667474, 0.35883409, 0.48304589, 0.13801311],\n",
       "       [0.76521855, 0.33391355, 0.52869645, 0.15304371],\n",
       "       [0.77242925, 0.33706004, 0.51963422, 0.14044168],\n",
       "       [0.76434981, 0.35581802, 0.51395936, 0.15814134],\n",
       "       [0.70779525, 0.31850786, 0.60162596, 0.1887454 ],\n",
       "       [0.69333409, 0.38518561, 0.57777841, 0.1925928 ],\n",
       "       [0.71524936, 0.40530797, 0.53643702, 0.19073316],\n",
       "       [0.75457341, 0.34913098, 0.52932761, 0.16893434],\n",
       "       [0.77530021, 0.28304611, 0.54147951, 0.15998258],\n",
       "       [0.72992443, 0.39103094, 0.53440896, 0.16944674],\n",
       "       [0.74714194, 0.33960997, 0.54337595, 0.17659719],\n",
       "       [0.72337118, 0.34195729, 0.57869695, 0.15782644],\n",
       "       [0.73260391, 0.36029701, 0.55245541, 0.1681386 ],\n",
       "       [0.76262994, 0.34186859, 0.52595168, 0.1577855 ],\n",
       "       [0.76986879, 0.35413965, 0.5081134 , 0.15397376],\n",
       "       [0.73544284, 0.35458851, 0.55158213, 0.1707278 ],\n",
       "       [0.73239618, 0.38547167, 0.53966034, 0.15418867],\n",
       "       [0.73446047, 0.37367287, 0.5411814 , 0.16750853],\n",
       "       [0.75728103, 0.3542121 , 0.52521104, 0.15878473],\n",
       "       [0.78258054, 0.38361791, 0.4603415 , 0.16879188],\n",
       "       [0.7431482 , 0.36505526, 0.5345452 , 0.16948994],\n",
       "       [0.65387747, 0.34250725, 0.62274045, 0.25947519],\n",
       "       [0.69052512, 0.32145135, 0.60718588, 0.22620651],\n",
       "       [0.71491405, 0.30207636, 0.59408351, 0.21145345],\n",
       "       [0.69276796, 0.31889319, 0.61579374, 0.1979337 ],\n",
       "       [0.68619022, 0.31670318, 0.61229281, 0.232249  ],\n",
       "       [0.70953708, 0.28008043, 0.61617694, 0.1960563 ],\n",
       "       [0.67054118, 0.34211284, 0.61580312, 0.23263673],\n",
       "       [0.71366557, 0.28351098, 0.61590317, 0.17597233],\n",
       "       [0.71414125, 0.26647062, 0.61821183, 0.19185884],\n",
       "       [0.69198788, 0.34599394, 0.58626751, 0.24027357],\n",
       "       [0.71562645, 0.3523084 , 0.56149152, 0.22019275],\n",
       "       [0.71576546, 0.30196356, 0.59274328, 0.21249287],\n",
       "       [0.71718148, 0.31640359, 0.58007326, 0.22148252],\n",
       "       [0.6925518 , 0.30375079, 0.60750157, 0.24300063],\n",
       "       [0.67767924, 0.32715549, 0.59589036, 0.28041899],\n",
       "       [0.69589887, 0.34794944, 0.57629125, 0.25008866],\n",
       "       [0.70610474, 0.3258945 , 0.59747324, 0.1955367 ],\n",
       "       [0.69299099, 0.34199555, 0.60299216, 0.19799743],\n",
       "       [0.70600618, 0.2383917 , 0.63265489, 0.21088496],\n",
       "       [0.72712585, 0.26661281, 0.60593821, 0.18178146],\n",
       "       [0.70558934, 0.32722984, 0.58287815, 0.23519645],\n",
       "       [0.68307923, 0.34153961, 0.59769433, 0.24395687],\n",
       "       [0.71486543, 0.25995106, 0.62202576, 0.18567933],\n",
       "       [0.73122464, 0.31338199, 0.56873028, 0.20892133],\n",
       "       [0.69595601, 0.3427843 , 0.59208198, 0.21813547],\n",
       "       [0.71529453, 0.31790868, 0.59607878, 0.17882363],\n",
       "       [0.72785195, 0.32870733, 0.56349829, 0.21131186],\n",
       "       [0.71171214, 0.35002236, 0.57170319, 0.21001342],\n",
       "       [0.69594002, 0.30447376, 0.60894751, 0.22835532],\n",
       "       [0.73089855, 0.30454106, 0.58877939, 0.1624219 ],\n",
       "       [0.72766159, 0.27533141, 0.59982915, 0.18683203],\n",
       "       [0.71578999, 0.34430405, 0.5798805 , 0.18121266],\n",
       "       [0.69417747, 0.30370264, 0.60740528, 0.2386235 ],\n",
       "       [0.72366005, 0.32162669, 0.58582004, 0.17230001],\n",
       "       [0.69385414, 0.29574111, 0.63698085, 0.15924521],\n",
       "       [0.73154399, 0.28501714, 0.57953485, 0.21851314],\n",
       "       [0.67017484, 0.36168166, 0.59571097, 0.2553047 ],\n",
       "       [0.69804799, 0.338117  , 0.59988499, 0.196326  ],\n",
       "       [0.71066905, 0.35533453, 0.56853524, 0.21320072],\n",
       "       [0.72415258, 0.32534391, 0.56672811, 0.22039426],\n",
       "       [0.69997037, 0.32386689, 0.58504986, 0.25073566],\n",
       "       [0.73337886, 0.32948905, 0.54206264, 0.24445962],\n",
       "       [0.69052512, 0.32145135, 0.60718588, 0.22620651],\n",
       "       [0.69193502, 0.32561648, 0.60035539, 0.23403685],\n",
       "       [0.68914871, 0.33943145, 0.58629069, 0.25714504],\n",
       "       [0.72155725, 0.32308533, 0.56001458, 0.24769876],\n",
       "       [0.72965359, 0.28954508, 0.57909015, 0.22005426],\n",
       "       [0.71653899, 0.3307103 , 0.57323119, 0.22047353],\n",
       "       [0.67467072, 0.36998072, 0.58761643, 0.25028107],\n",
       "       [0.69025916, 0.35097923, 0.5966647 , 0.21058754]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow.keras\n",
    "\n",
    "TR_XN = keras.utils.normalize(TR_X)\n",
    "TR_XN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lUx516tc8E6Q"
   },
   "source": [
    "> ## 4) One Hot Encoding with sklearn Package & Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9lm5fWHmIgOd"
   },
   "source": [
    "* ['setosa', 'virginica', 'virsicolor'] to [0, 1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 147
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1259,
     "status": "ok",
     "timestamp": 1574488689252,
     "user": {
      "displayName": "이정구",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCJOG97rou_jpwsvziQA7tTe2JnwoyotzfHjlO2=s64",
      "userId": "17400242557229223804"
     },
     "user_tz": -540
    },
    "id": "iFhNzeIMbXyV",
    "outputId": "accda076-ca7a-464c-bd82-5c554b51fcd3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder =  LabelEncoder()\n",
    "encoder.fit(TR_Y)\n",
    "TR_YL = encoder.transform(TR_Y)\n",
    "\n",
    "TR_YL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k7LEIdH3JAtw"
   },
   "source": [
    "* to_categorical( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hxdLQPuuZQ13"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TR_YO = keras.utils.to_categorical(TR_YL)\n",
    "\n",
    "TR_YO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8zievJ6A8dQc"
   },
   "source": [
    "> ## 5) Train & Test Split with sklearn Package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CT8iuCjxJdIn"
   },
   "source": [
    "* 8:2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 656,
     "status": "ok",
     "timestamp": 1574488798311,
     "user": {
      "displayName": "이정구",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCJOG97rou_jpwsvziQA7tTe2JnwoyotzfHjlO2=s64",
      "userId": "17400242557229223804"
     },
     "user_tz": -540
    },
    "id": "WIiSOik0dGct",
    "outputId": "e884bd6f-7a33-4646-c987-2710b9607c36"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((120, 4), (30, 4), (120, 3), (30, 3))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(TR_XN, TR_YO, \n",
    "                                                    test_size = 0.2, \n",
    "                                                    random_state = 2045) \n",
    "\n",
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_WuSRwBd8oxV"
   },
   "source": [
    "# II. Keras Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5RHmF1kr9heb"
   },
   "source": [
    "> ## 1) Keras models & layers Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K0DN73_6duqG"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-rDeiCMV-eKV"
   },
   "source": [
    "> ## 2) Model Define"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1ix6_3Tk-yWW"
   },
   "source": [
    "* 모델 신경망 구조 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DJ8uzQMSemA-"
   },
   "outputs": [],
   "source": [
    "Model_iris = models.Sequential()\n",
    "\n",
    "Model_iris.add(layers.Dense(16, activation = 'relu', input_shape = (4,)))\n",
    "Model_iris.add(layers.Dense(8, activation = 'relu'))\n",
    "Model_iris.add(layers.Dense(3, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U0vAUcHx-zdW"
   },
   "source": [
    "* 모델 구조 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1167,
     "status": "ok",
     "timestamp": 1574489049094,
     "user": {
      "displayName": "이정구",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCJOG97rou_jpwsvziQA7tTe2JnwoyotzfHjlO2=s64",
      "userId": "17400242557229223804"
     },
     "user_tz": -540
    },
    "id": "1sfOdgz2jQeM",
    "outputId": "e6bbe7ff-5b32-4256-a65a-e3ba2c4b463d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 27        \n",
      "=================================================================\n",
      "Total params: 243\n",
      "Trainable params: 243\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Model_iris.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3jTRDB6y-5Mx"
   },
   "source": [
    "> ## 3) Model Compile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iixRFcKM_PUJ"
   },
   "source": [
    "* 모델 학습방법 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qe9CXqyKjHhI"
   },
   "outputs": [],
   "source": [
    "Model_iris.compile(loss = 'categorical_crossentropy',\n",
    "                   optimizer = 'rmsprop',\n",
    "                   metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y7zLg_l4_BvM"
   },
   "source": [
    "> ## 4) Model Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BLjGgSpG_UiT"
   },
   "source": [
    "* 모델 학습 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1J-5DPpUjKPZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 120 samples, validate on 30 samples\n",
      "Epoch 1/500\n",
      "120/120 [==============================] - 0s 449us/sample - loss: 0.0991 - accuracy: 0.9500 - val_loss: 0.0312 - val_accuracy: 1.0000\n",
      "Epoch 2/500\n",
      "120/120 [==============================] - 0s 407us/sample - loss: 0.0973 - accuracy: 0.9417 - val_loss: 0.0323 - val_accuracy: 1.0000\n",
      "Epoch 3/500\n",
      "120/120 [==============================] - 0s 524us/sample - loss: 0.1004 - accuracy: 0.9417 - val_loss: 0.0297 - val_accuracy: 1.0000\n",
      "Epoch 4/500\n",
      "120/120 [==============================] - 0s 590us/sample - loss: 0.0966 - accuracy: 0.9417 - val_loss: 0.0334 - val_accuracy: 1.0000\n",
      "Epoch 5/500\n",
      "120/120 [==============================] - 0s 474us/sample - loss: 0.0966 - accuracy: 0.9500 - val_loss: 0.0292 - val_accuracy: 1.0000\n",
      "Epoch 6/500\n",
      "120/120 [==============================] - 0s 557us/sample - loss: 0.0961 - accuracy: 0.9417 - val_loss: 0.0291 - val_accuracy: 1.0000\n",
      "Epoch 7/500\n",
      "120/120 [==============================] - 0s 391us/sample - loss: 0.0988 - accuracy: 0.9500 - val_loss: 0.0290 - val_accuracy: 1.0000\n",
      "Epoch 8/500\n",
      "120/120 [==============================] - 0s 503us/sample - loss: 0.0998 - accuracy: 0.9417 - val_loss: 0.0298 - val_accuracy: 1.0000\n",
      "Epoch 9/500\n",
      "120/120 [==============================] - 0s 382us/sample - loss: 0.0964 - accuracy: 0.9417 - val_loss: 0.0290 - val_accuracy: 1.0000\n",
      "Epoch 10/500\n",
      "120/120 [==============================] - 0s 424us/sample - loss: 0.0994 - accuracy: 0.9417 - val_loss: 0.0292 - val_accuracy: 1.0000\n",
      "Epoch 11/500\n",
      "120/120 [==============================] - 0s 524us/sample - loss: 0.0998 - accuracy: 0.9417 - val_loss: 0.0289 - val_accuracy: 1.0000\n",
      "Epoch 12/500\n",
      "120/120 [==============================] - 0s 407us/sample - loss: 0.0982 - accuracy: 0.9417 - val_loss: 0.0308 - val_accuracy: 1.0000\n",
      "Epoch 13/500\n",
      "120/120 [==============================] - 0s 407us/sample - loss: 0.1002 - accuracy: 0.9417 - val_loss: 0.0359 - val_accuracy: 1.0000\n",
      "Epoch 14/500\n",
      "120/120 [==============================] - 0s 382us/sample - loss: 0.0994 - accuracy: 0.9500 - val_loss: 0.0288 - val_accuracy: 1.0000\n",
      "Epoch 15/500\n",
      "120/120 [==============================] - 0s 441us/sample - loss: 0.0945 - accuracy: 0.9500 - val_loss: 0.0333 - val_accuracy: 1.0000\n",
      "Epoch 16/500\n",
      "120/120 [==============================] - 0s 424us/sample - loss: 0.0979 - accuracy: 0.9500 - val_loss: 0.0297 - val_accuracy: 1.0000\n",
      "Epoch 17/500\n",
      "120/120 [==============================] - 0s 407us/sample - loss: 0.0986 - accuracy: 0.9417 - val_loss: 0.0295 - val_accuracy: 1.0000\n",
      "Epoch 18/500\n",
      "120/120 [==============================] - 0s 499us/sample - loss: 0.0956 - accuracy: 0.9417 - val_loss: 0.0294 - val_accuracy: 1.0000\n",
      "Epoch 19/500\n",
      "120/120 [==============================] - 0s 432us/sample - loss: 0.0975 - accuracy: 0.9583 - val_loss: 0.0290 - val_accuracy: 1.0000\n",
      "Epoch 20/500\n",
      "120/120 [==============================] - 0s 542us/sample - loss: 0.0967 - accuracy: 0.9417 - val_loss: 0.0293 - val_accuracy: 1.0000\n",
      "Epoch 21/500\n",
      "120/120 [==============================] - 0s 557us/sample - loss: 0.0940 - accuracy: 0.9417 - val_loss: 0.0347 - val_accuracy: 1.0000\n",
      "Epoch 22/500\n",
      "120/120 [==============================] - 0s 497us/sample - loss: 0.0981 - accuracy: 0.9583 - val_loss: 0.0290 - val_accuracy: 1.0000\n",
      "Epoch 23/500\n",
      "120/120 [==============================] - 0s 482us/sample - loss: 0.0961 - accuracy: 0.9417 - val_loss: 0.0306 - val_accuracy: 1.0000\n",
      "Epoch 24/500\n",
      "120/120 [==============================] - 0s 722us/sample - loss: 0.0989 - accuracy: 0.9417 - val_loss: 0.0286 - val_accuracy: 1.0000\n",
      "Epoch 25/500\n",
      "120/120 [==============================] - 0s 1ms/sample - loss: 0.0990 - accuracy: 0.9417 - val_loss: 0.0299 - val_accuracy: 1.0000\n",
      "Epoch 26/500\n",
      "120/120 [==============================] - 0s 865us/sample - loss: 0.0983 - accuracy: 0.9417 - val_loss: 0.0290 - val_accuracy: 1.0000\n",
      "Epoch 27/500\n",
      "120/120 [==============================] - 0s 885us/sample - loss: 0.0946 - accuracy: 0.9417 - val_loss: 0.0286 - val_accuracy: 1.0000\n",
      "Epoch 28/500\n",
      "120/120 [==============================] - 0s 532us/sample - loss: 0.0929 - accuracy: 0.9583 - val_loss: 0.0299 - val_accuracy: 1.0000\n",
      "Epoch 29/500\n",
      "120/120 [==============================] - 0s 565us/sample - loss: 0.0997 - accuracy: 0.9417 - val_loss: 0.0285 - val_accuracy: 1.0000\n",
      "Epoch 30/500\n",
      "120/120 [==============================] - 0s 505us/sample - loss: 0.0985 - accuracy: 0.9417 - val_loss: 0.0323 - val_accuracy: 1.0000\n",
      "Epoch 31/500\n",
      "120/120 [==============================] - 0s 546us/sample - loss: 0.0957 - accuracy: 0.9417 - val_loss: 0.0288 - val_accuracy: 1.0000\n",
      "Epoch 32/500\n",
      "120/120 [==============================] - 0s 482us/sample - loss: 0.0972 - accuracy: 0.9417 - val_loss: 0.0294 - val_accuracy: 1.0000\n",
      "Epoch 33/500\n",
      "120/120 [==============================] - 0s 449us/sample - loss: 0.1005 - accuracy: 0.9417 - val_loss: 0.0286 - val_accuracy: 1.0000\n",
      "Epoch 34/500\n",
      "120/120 [==============================] - 0s 409us/sample - loss: 0.1007 - accuracy: 0.9417 - val_loss: 0.0283 - val_accuracy: 1.0000\n",
      "Epoch 35/500\n",
      "120/120 [==============================] - 0s 416us/sample - loss: 0.0984 - accuracy: 0.9417 - val_loss: 0.0284 - val_accuracy: 1.0000\n",
      "Epoch 36/500\n",
      "120/120 [==============================] - 0s 416us/sample - loss: 0.0970 - accuracy: 0.9417 - val_loss: 0.0293 - val_accuracy: 1.0000\n",
      "Epoch 37/500\n",
      "120/120 [==============================] - 0s 490us/sample - loss: 0.1005 - accuracy: 0.9417 - val_loss: 0.0289 - val_accuracy: 1.0000\n",
      "Epoch 38/500\n",
      "120/120 [==============================] - 0s 457us/sample - loss: 0.1002 - accuracy: 0.9417 - val_loss: 0.0323 - val_accuracy: 1.0000\n",
      "Epoch 39/500\n",
      "120/120 [==============================] - 0s 490us/sample - loss: 0.0948 - accuracy: 0.9583 - val_loss: 0.0283 - val_accuracy: 1.0000\n",
      "Epoch 40/500\n",
      "120/120 [==============================] - 0s 543us/sample - loss: 0.0964 - accuracy: 0.9500 - val_loss: 0.0323 - val_accuracy: 1.0000\n",
      "Epoch 41/500\n",
      "120/120 [==============================] - 0s 391us/sample - loss: 0.0991 - accuracy: 0.9500 - val_loss: 0.0283 - val_accuracy: 1.0000\n",
      "Epoch 42/500\n",
      "120/120 [==============================] - 0s 449us/sample - loss: 0.0956 - accuracy: 0.9417 - val_loss: 0.0286 - val_accuracy: 1.0000\n",
      "Epoch 43/500\n",
      "120/120 [==============================] - 0s 491us/sample - loss: 0.0971 - accuracy: 0.9417 - val_loss: 0.0297 - val_accuracy: 1.0000\n",
      "Epoch 44/500\n",
      "120/120 [==============================] - 0s 474us/sample - loss: 0.0978 - accuracy: 0.9417 - val_loss: 0.0281 - val_accuracy: 1.0000\n",
      "Epoch 45/500\n",
      "120/120 [==============================] - 0s 431us/sample - loss: 0.1029 - accuracy: 0.9500 - val_loss: 0.0282 - val_accuracy: 1.0000\n",
      "Epoch 46/500\n",
      "120/120 [==============================] - 0s 434us/sample - loss: 0.0959 - accuracy: 0.9417 - val_loss: 0.0294 - val_accuracy: 1.0000\n",
      "Epoch 47/500\n",
      "120/120 [==============================] - 0s 608us/sample - loss: 0.0951 - accuracy: 0.9500 - val_loss: 0.0369 - val_accuracy: 1.0000\n",
      "Epoch 48/500\n",
      "120/120 [==============================] - 0s 457us/sample - loss: 0.1003 - accuracy: 0.9500 - val_loss: 0.0318 - val_accuracy: 1.0000\n",
      "Epoch 49/500\n",
      "120/120 [==============================] - 0s 789us/sample - loss: 0.1032 - accuracy: 0.9417 - val_loss: 0.0284 - val_accuracy: 1.0000\n",
      "Epoch 50/500\n",
      "120/120 [==============================] - 0s 482us/sample - loss: 0.0956 - accuracy: 0.9500 - val_loss: 0.0280 - val_accuracy: 1.0000\n",
      "Epoch 51/500\n",
      "120/120 [==============================] - 0s 573us/sample - loss: 0.0965 - accuracy: 0.9500 - val_loss: 0.0283 - val_accuracy: 1.0000\n",
      "Epoch 52/500\n",
      "120/120 [==============================] - 0s 614us/sample - loss: 0.0955 - accuracy: 0.9500 - val_loss: 0.0294 - val_accuracy: 1.0000\n",
      "Epoch 53/500\n",
      "120/120 [==============================] - 0s 590us/sample - loss: 0.1002 - accuracy: 0.9417 - val_loss: 0.0285 - val_accuracy: 1.0000\n",
      "Epoch 54/500\n",
      "120/120 [==============================] - 0s 757us/sample - loss: 0.1019 - accuracy: 0.9417 - val_loss: 0.0327 - val_accuracy: 1.0000\n",
      "Epoch 55/500\n",
      "120/120 [==============================] - 0s 407us/sample - loss: 0.1006 - accuracy: 0.9500 - val_loss: 0.0287 - val_accuracy: 1.0000\n",
      "Epoch 56/500\n",
      "120/120 [==============================] - 0s 474us/sample - loss: 0.0951 - accuracy: 0.9500 - val_loss: 0.0292 - val_accuracy: 1.0000\n",
      "Epoch 57/500\n",
      "120/120 [==============================] - 0s 466us/sample - loss: 0.1005 - accuracy: 0.9500 - val_loss: 0.0297 - val_accuracy: 1.0000\n",
      "Epoch 58/500\n",
      "120/120 [==============================] - 0s 578us/sample - loss: 0.0911 - accuracy: 0.9583 - val_loss: 0.0310 - val_accuracy: 1.0000\n",
      "Epoch 59/500\n",
      "120/120 [==============================] - 0s 582us/sample - loss: 0.0977 - accuracy: 0.9417 - val_loss: 0.0278 - val_accuracy: 1.0000\n",
      "Epoch 60/500\n",
      "120/120 [==============================] - 0s 615us/sample - loss: 0.0984 - accuracy: 0.9417 - val_loss: 0.0296 - val_accuracy: 1.0000\n",
      "Epoch 61/500\n",
      "120/120 [==============================] - 0s 392us/sample - loss: 0.0935 - accuracy: 0.9417 - val_loss: 0.0293 - val_accuracy: 1.0000\n",
      "Epoch 62/500\n",
      "120/120 [==============================] - 0s 573us/sample - loss: 0.0990 - accuracy: 0.9417 - val_loss: 0.0280 - val_accuracy: 1.0000\n",
      "Epoch 63/500\n",
      "120/120 [==============================] - 0s 465us/sample - loss: 0.0944 - accuracy: 0.9417 - val_loss: 0.0290 - val_accuracy: 1.0000\n",
      "Epoch 64/500\n",
      "120/120 [==============================] - 0s 690us/sample - loss: 0.0970 - accuracy: 0.9500 - val_loss: 0.0280 - val_accuracy: 1.0000\n",
      "Epoch 65/500\n",
      "120/120 [==============================] - 0s 457us/sample - loss: 0.0993 - accuracy: 0.9500 - val_loss: 0.0317 - val_accuracy: 1.0000\n",
      "Epoch 66/500\n",
      "120/120 [==============================] - 0s 489us/sample - loss: 0.0989 - accuracy: 0.9583 - val_loss: 0.0276 - val_accuracy: 1.0000\n",
      "Epoch 67/500\n",
      "120/120 [==============================] - 0s 482us/sample - loss: 0.0962 - accuracy: 0.9417 - val_loss: 0.0307 - val_accuracy: 1.0000\n",
      "Epoch 68/500\n",
      "120/120 [==============================] - 0s 457us/sample - loss: 0.0977 - accuracy: 0.9417 - val_loss: 0.0281 - val_accuracy: 1.0000\n",
      "Epoch 69/500\n",
      "120/120 [==============================] - 0s 499us/sample - loss: 0.0977 - accuracy: 0.9417 - val_loss: 0.0276 - val_accuracy: 1.0000\n",
      "Epoch 70/500\n",
      "120/120 [==============================] - 0s 399us/sample - loss: 0.0928 - accuracy: 0.9583 - val_loss: 0.0306 - val_accuracy: 1.0000\n",
      "Epoch 71/500\n",
      "120/120 [==============================] - 0s 407us/sample - loss: 0.0990 - accuracy: 0.9583 - val_loss: 0.0276 - val_accuracy: 1.0000\n",
      "Epoch 72/500\n",
      "120/120 [==============================] - 0s 370us/sample - loss: 0.0963 - accuracy: 0.9417 - val_loss: 0.0275 - val_accuracy: 1.0000\n",
      "Epoch 73/500\n",
      "120/120 [==============================] - 0s 374us/sample - loss: 0.0988 - accuracy: 0.9500 - val_loss: 0.0296 - val_accuracy: 1.0000\n",
      "Epoch 74/500\n",
      "120/120 [==============================] - 0s 407us/sample - loss: 0.1001 - accuracy: 0.9417 - val_loss: 0.0274 - val_accuracy: 1.0000\n",
      "Epoch 75/500\n",
      "120/120 [==============================] - 0s 416us/sample - loss: 0.0959 - accuracy: 0.9417 - val_loss: 0.0275 - val_accuracy: 1.0000\n",
      "Epoch 76/500\n",
      "120/120 [==============================] - 0s 382us/sample - loss: 0.0959 - accuracy: 0.9417 - val_loss: 0.0275 - val_accuracy: 1.0000\n",
      "Epoch 77/500\n",
      "120/120 [==============================] - 0s 521us/sample - loss: 0.0987 - accuracy: 0.9500 - val_loss: 0.0279 - val_accuracy: 1.0000\n",
      "Epoch 78/500\n",
      "120/120 [==============================] - 0s 432us/sample - loss: 0.0953 - accuracy: 0.9417 - val_loss: 0.0324 - val_accuracy: 1.0000\n",
      "Epoch 79/500\n",
      "120/120 [==============================] - 0s 391us/sample - loss: 0.1025 - accuracy: 0.9500 - val_loss: 0.0276 - val_accuracy: 1.0000\n",
      "Epoch 80/500\n",
      "120/120 [==============================] - 0s 390us/sample - loss: 0.1001 - accuracy: 0.9417 - val_loss: 0.0275 - val_accuracy: 1.0000\n",
      "Epoch 81/500\n",
      "120/120 [==============================] - 0s 459us/sample - loss: 0.0978 - accuracy: 0.9417 - val_loss: 0.0277 - val_accuracy: 1.0000\n",
      "Epoch 82/500\n",
      "120/120 [==============================] - 0s 449us/sample - loss: 0.0932 - accuracy: 0.9417 - val_loss: 0.0282 - val_accuracy: 1.0000\n",
      "Epoch 83/500\n",
      "120/120 [==============================] - 0s 490us/sample - loss: 0.0940 - accuracy: 0.9417 - val_loss: 0.0282 - val_accuracy: 1.0000\n",
      "Epoch 84/500\n",
      "120/120 [==============================] - 0s 424us/sample - loss: 0.1009 - accuracy: 0.9417 - val_loss: 0.0273 - val_accuracy: 1.0000\n",
      "Epoch 85/500\n",
      "120/120 [==============================] - 0s 475us/sample - loss: 0.0919 - accuracy: 0.9417 - val_loss: 0.0342 - val_accuracy: 1.0000\n",
      "Epoch 86/500\n",
      "120/120 [==============================] - 0s 465us/sample - loss: 0.0980 - accuracy: 0.9500 - val_loss: 0.0328 - val_accuracy: 1.0000\n",
      "Epoch 87/500\n",
      "120/120 [==============================] - 0s 399us/sample - loss: 0.0985 - accuracy: 0.9500 - val_loss: 0.0273 - val_accuracy: 1.0000\n",
      "Epoch 88/500\n",
      "120/120 [==============================] - 0s 374us/sample - loss: 0.0936 - accuracy: 0.9500 - val_loss: 0.0281 - val_accuracy: 1.0000\n",
      "Epoch 89/500\n",
      "120/120 [==============================] - 0s 482us/sample - loss: 0.0987 - accuracy: 0.9417 - val_loss: 0.0294 - val_accuracy: 1.0000\n",
      "Epoch 90/500\n",
      "120/120 [==============================] - 0s 540us/sample - loss: 0.0993 - accuracy: 0.9417 - val_loss: 0.0273 - val_accuracy: 1.0000\n",
      "Epoch 91/500\n",
      "120/120 [==============================] - 0s 523us/sample - loss: 0.0967 - accuracy: 0.9417 - val_loss: 0.0272 - val_accuracy: 1.0000\n",
      "Epoch 92/500\n",
      "120/120 [==============================] - 0s 432us/sample - loss: 0.0984 - accuracy: 0.9500 - val_loss: 0.0273 - val_accuracy: 1.0000\n",
      "Epoch 93/500\n",
      "120/120 [==============================] - 0s 407us/sample - loss: 0.0990 - accuracy: 0.9500 - val_loss: 0.0290 - val_accuracy: 1.0000\n",
      "Epoch 94/500\n",
      "120/120 [==============================] - 0s 404us/sample - loss: 0.0985 - accuracy: 0.9417 - val_loss: 0.0282 - val_accuracy: 1.0000\n",
      "Epoch 95/500\n",
      "120/120 [==============================] - 0s 407us/sample - loss: 0.0959 - accuracy: 0.9417 - val_loss: 0.0270 - val_accuracy: 1.0000\n",
      "Epoch 96/500\n",
      "120/120 [==============================] - 0s 391us/sample - loss: 0.0994 - accuracy: 0.9417 - val_loss: 0.0285 - val_accuracy: 1.0000\n",
      "Epoch 97/500\n",
      "120/120 [==============================] - 0s 382us/sample - loss: 0.0974 - accuracy: 0.9417 - val_loss: 0.0310 - val_accuracy: 1.0000\n",
      "Epoch 98/500\n",
      "120/120 [==============================] - 0s 465us/sample - loss: 0.0962 - accuracy: 0.9500 - val_loss: 0.0309 - val_accuracy: 1.0000\n",
      "Epoch 99/500\n",
      "120/120 [==============================] - 0s 374us/sample - loss: 0.0962 - accuracy: 0.9417 - val_loss: 0.0270 - val_accuracy: 1.0000\n",
      "Epoch 100/500\n",
      "120/120 [==============================] - 0s 382us/sample - loss: 0.0940 - accuracy: 0.9417 - val_loss: 0.0299 - val_accuracy: 1.0000\n",
      "Epoch 101/500\n",
      "120/120 [==============================] - 0s 375us/sample - loss: 0.0949 - accuracy: 0.9417 - val_loss: 0.0272 - val_accuracy: 1.0000\n",
      "Epoch 102/500\n",
      "120/120 [==============================] - 0s 374us/sample - loss: 0.0990 - accuracy: 0.9417 - val_loss: 0.0278 - val_accuracy: 1.0000\n",
      "Epoch 103/500\n",
      "120/120 [==============================] - 0s 416us/sample - loss: 0.0959 - accuracy: 0.9500 - val_loss: 0.0270 - val_accuracy: 1.0000\n",
      "Epoch 104/500\n",
      "120/120 [==============================] - 0s 399us/sample - loss: 0.0965 - accuracy: 0.9417 - val_loss: 0.0269 - val_accuracy: 1.0000\n",
      "Epoch 105/500\n",
      "120/120 [==============================] - 0s 490us/sample - loss: 0.0982 - accuracy: 0.9417 - val_loss: 0.0270 - val_accuracy: 1.0000\n",
      "Epoch 106/500\n",
      "120/120 [==============================] - 0s 516us/sample - loss: 0.0959 - accuracy: 0.9417 - val_loss: 0.0269 - val_accuracy: 1.0000\n",
      "Epoch 107/500\n",
      "120/120 [==============================] - 0s 432us/sample - loss: 0.0956 - accuracy: 0.9417 - val_loss: 0.0288 - val_accuracy: 1.0000\n",
      "Epoch 108/500\n",
      "120/120 [==============================] - 0s 398us/sample - loss: 0.0970 - accuracy: 0.9417 - val_loss: 0.0268 - val_accuracy: 1.0000\n",
      "Epoch 109/500\n",
      "120/120 [==============================] - 0s 457us/sample - loss: 0.0932 - accuracy: 0.9417 - val_loss: 0.0268 - val_accuracy: 1.0000\n",
      "Epoch 110/500\n",
      "120/120 [==============================] - 0s 524us/sample - loss: 0.0962 - accuracy: 0.9417 - val_loss: 0.0275 - val_accuracy: 1.0000\n",
      "Epoch 111/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 451us/sample - loss: 0.0944 - accuracy: 0.9417 - val_loss: 0.0278 - val_accuracy: 1.0000\n",
      "Epoch 112/500\n",
      "120/120 [==============================] - 0s 419us/sample - loss: 0.0970 - accuracy: 0.9500 - val_loss: 0.0272 - val_accuracy: 1.0000\n",
      "Epoch 113/500\n",
      "120/120 [==============================] - 0s 391us/sample - loss: 0.0918 - accuracy: 0.9500 - val_loss: 0.0302 - val_accuracy: 1.0000\n",
      "Epoch 114/500\n",
      "120/120 [==============================] - 0s 401us/sample - loss: 0.0967 - accuracy: 0.9417 - val_loss: 0.0278 - val_accuracy: 1.0000\n",
      "Epoch 115/500\n",
      "120/120 [==============================] - 0s 499us/sample - loss: 0.0967 - accuracy: 0.9417 - val_loss: 0.0289 - val_accuracy: 1.0000\n",
      "Epoch 116/500\n",
      "120/120 [==============================] - 0s 423us/sample - loss: 0.0958 - accuracy: 0.9500 - val_loss: 0.0269 - val_accuracy: 1.0000\n",
      "Epoch 117/500\n",
      "120/120 [==============================] - 0s 457us/sample - loss: 0.0977 - accuracy: 0.9417 - val_loss: 0.0301 - val_accuracy: 1.0000\n",
      "Epoch 118/500\n",
      "120/120 [==============================] - 0s 407us/sample - loss: 0.0957 - accuracy: 0.9500 - val_loss: 0.0276 - val_accuracy: 1.0000\n",
      "Epoch 119/500\n",
      "120/120 [==============================] - 0s 382us/sample - loss: 0.0972 - accuracy: 0.9417 - val_loss: 0.0267 - val_accuracy: 1.0000\n",
      "Epoch 120/500\n",
      "120/120 [==============================] - 0s 382us/sample - loss: 0.0980 - accuracy: 0.9417 - val_loss: 0.0269 - val_accuracy: 1.0000\n",
      "Epoch 121/500\n",
      "120/120 [==============================] - 0s 441us/sample - loss: 0.0980 - accuracy: 0.9500 - val_loss: 0.0266 - val_accuracy: 1.0000\n",
      "Epoch 122/500\n",
      "120/120 [==============================] - 0s 415us/sample - loss: 0.0981 - accuracy: 0.9500 - val_loss: 0.0267 - val_accuracy: 1.0000\n",
      "Epoch 123/500\n",
      "120/120 [==============================] - 0s 499us/sample - loss: 0.0946 - accuracy: 0.9417 - val_loss: 0.0273 - val_accuracy: 1.0000\n",
      "Epoch 124/500\n",
      "120/120 [==============================] - 0s 457us/sample - loss: 0.0986 - accuracy: 0.9417 - val_loss: 0.0267 - val_accuracy: 1.0000\n",
      "Epoch 125/500\n",
      "120/120 [==============================] - 0s 472us/sample - loss: 0.0962 - accuracy: 0.9500 - val_loss: 0.0269 - val_accuracy: 1.0000\n",
      "Epoch 126/500\n",
      "120/120 [==============================] - 0s 496us/sample - loss: 0.0935 - accuracy: 0.9417 - val_loss: 0.0265 - val_accuracy: 1.0000\n",
      "Epoch 127/500\n",
      "120/120 [==============================] - 0s 499us/sample - loss: 0.0959 - accuracy: 0.9417 - val_loss: 0.0277 - val_accuracy: 1.0000\n",
      "Epoch 128/500\n",
      "120/120 [==============================] - 0s 449us/sample - loss: 0.0896 - accuracy: 0.9500 - val_loss: 0.0291 - val_accuracy: 1.0000\n",
      "Epoch 129/500\n",
      "120/120 [==============================] - 0s 449us/sample - loss: 0.0971 - accuracy: 0.9500 - val_loss: 0.0271 - val_accuracy: 1.0000\n",
      "Epoch 130/500\n",
      "120/120 [==============================] - 0s 404us/sample - loss: 0.0961 - accuracy: 0.9417 - val_loss: 0.0272 - val_accuracy: 1.0000\n",
      "Epoch 131/500\n",
      "120/120 [==============================] - 0s 540us/sample - loss: 0.0976 - accuracy: 0.9417 - val_loss: 0.0264 - val_accuracy: 1.0000\n",
      "Epoch 132/500\n",
      "120/120 [==============================] - 0s 416us/sample - loss: 0.0910 - accuracy: 0.9417 - val_loss: 0.0313 - val_accuracy: 1.0000\n",
      "Epoch 133/500\n",
      "120/120 [==============================] - 0s 489us/sample - loss: 0.0965 - accuracy: 0.9500 - val_loss: 0.0293 - val_accuracy: 1.0000\n",
      "Epoch 134/500\n",
      "120/120 [==============================] - 0s 565us/sample - loss: 0.0971 - accuracy: 0.9500 - val_loss: 0.0264 - val_accuracy: 1.0000\n",
      "Epoch 135/500\n",
      "120/120 [==============================] - 0s 597us/sample - loss: 0.0963 - accuracy: 0.9417 - val_loss: 0.0276 - val_accuracy: 1.0000\n",
      "Epoch 136/500\n",
      "120/120 [==============================] - 0s 559us/sample - loss: 0.0985 - accuracy: 0.9417 - val_loss: 0.0264 - val_accuracy: 1.0000\n",
      "Epoch 137/500\n",
      "120/120 [==============================] - 0s 490us/sample - loss: 0.0987 - accuracy: 0.9500 - val_loss: 0.0266 - val_accuracy: 1.0000\n",
      "Epoch 138/500\n",
      "120/120 [==============================] - 0s 457us/sample - loss: 0.0909 - accuracy: 0.9417 - val_loss: 0.0305 - val_accuracy: 1.0000\n",
      "Epoch 139/500\n",
      "120/120 [==============================] - 0s 496us/sample - loss: 0.0982 - accuracy: 0.9500 - val_loss: 0.0264 - val_accuracy: 1.0000\n",
      "Epoch 140/500\n",
      "120/120 [==============================] - 0s 433us/sample - loss: 0.0941 - accuracy: 0.9500 - val_loss: 0.0287 - val_accuracy: 1.0000\n",
      "Epoch 141/500\n",
      "120/120 [==============================] - 0s 388us/sample - loss: 0.0981 - accuracy: 0.9417 - val_loss: 0.0263 - val_accuracy: 1.0000\n",
      "Epoch 142/500\n",
      "120/120 [==============================] - 0s 382us/sample - loss: 0.0936 - accuracy: 0.9417 - val_loss: 0.0285 - val_accuracy: 1.0000\n",
      "Epoch 143/500\n",
      "120/120 [==============================] - 0s 399us/sample - loss: 0.0978 - accuracy: 0.9500 - val_loss: 0.0262 - val_accuracy: 1.0000\n",
      "Epoch 144/500\n",
      "120/120 [==============================] - 0s 423us/sample - loss: 0.0972 - accuracy: 0.9500 - val_loss: 0.0270 - val_accuracy: 1.0000\n",
      "Epoch 145/500\n",
      "120/120 [==============================] - 0s 491us/sample - loss: 0.0918 - accuracy: 0.9417 - val_loss: 0.0276 - val_accuracy: 1.0000\n",
      "Epoch 146/500\n",
      "120/120 [==============================] - 0s 375us/sample - loss: 0.0948 - accuracy: 0.9417 - val_loss: 0.0299 - val_accuracy: 1.0000\n",
      "Epoch 147/500\n",
      "120/120 [==============================] - 0s 407us/sample - loss: 0.0952 - accuracy: 0.9417 - val_loss: 0.0265 - val_accuracy: 1.0000\n",
      "Epoch 148/500\n",
      "120/120 [==============================] - 0s 357us/sample - loss: 0.0947 - accuracy: 0.9417 - val_loss: 0.0273 - val_accuracy: 1.0000\n",
      "Epoch 149/500\n",
      "120/120 [==============================] - 0s 372us/sample - loss: 0.0959 - accuracy: 0.9417 - val_loss: 0.0264 - val_accuracy: 1.0000\n",
      "Epoch 150/500\n",
      "120/120 [==============================] - 0s 367us/sample - loss: 0.0999 - accuracy: 0.9417 - val_loss: 0.0261 - val_accuracy: 1.0000\n",
      "Epoch 151/500\n",
      "120/120 [==============================] - 0s 357us/sample - loss: 0.0946 - accuracy: 0.9417 - val_loss: 0.0261 - val_accuracy: 1.0000\n",
      "Epoch 152/500\n",
      "120/120 [==============================] - 0s 391us/sample - loss: 0.0972 - accuracy: 0.9417 - val_loss: 0.0266 - val_accuracy: 1.0000\n",
      "Epoch 153/500\n",
      "120/120 [==============================] - 0s 391us/sample - loss: 0.0934 - accuracy: 0.9417 - val_loss: 0.0282 - val_accuracy: 1.0000\n",
      "Epoch 154/500\n",
      "120/120 [==============================] - 0s 482us/sample - loss: 0.0881 - accuracy: 0.9500 - val_loss: 0.0292 - val_accuracy: 1.0000\n",
      "Epoch 155/500\n",
      "120/120 [==============================] - 0s 407us/sample - loss: 0.0973 - accuracy: 0.9500 - val_loss: 0.0262 - val_accuracy: 1.0000\n",
      "Epoch 156/500\n",
      "120/120 [==============================] - 0s 410us/sample - loss: 0.0980 - accuracy: 0.9417 - val_loss: 0.0261 - val_accuracy: 1.0000\n",
      "Epoch 157/500\n",
      "120/120 [==============================] - 0s 490us/sample - loss: 0.0998 - accuracy: 0.9417 - val_loss: 0.0294 - val_accuracy: 1.0000\n",
      "Epoch 158/500\n",
      "120/120 [==============================] - 0s 499us/sample - loss: 0.0957 - accuracy: 0.9417 - val_loss: 0.0263 - val_accuracy: 1.0000\n",
      "Epoch 159/500\n",
      "120/120 [==============================] - 0s 582us/sample - loss: 0.0916 - accuracy: 0.9417 - val_loss: 0.0277 - val_accuracy: 1.0000\n",
      "Epoch 160/500\n",
      "120/120 [==============================] - 0s 416us/sample - loss: 0.0980 - accuracy: 0.9417 - val_loss: 0.0260 - val_accuracy: 1.0000\n",
      "Epoch 161/500\n",
      "120/120 [==============================] - 0s 423us/sample - loss: 0.0977 - accuracy: 0.9417 - val_loss: 0.0260 - val_accuracy: 1.0000\n",
      "Epoch 162/500\n",
      "120/120 [==============================] - 0s 416us/sample - loss: 0.0928 - accuracy: 0.9417 - val_loss: 0.0275 - val_accuracy: 1.0000\n",
      "Epoch 163/500\n",
      "120/120 [==============================] - 0s 449us/sample - loss: 0.0998 - accuracy: 0.9500 - val_loss: 0.0260 - val_accuracy: 1.0000\n",
      "Epoch 164/500\n",
      "120/120 [==============================] - 0s 391us/sample - loss: 0.0949 - accuracy: 0.9417 - val_loss: 0.0332 - val_accuracy: 1.0000\n",
      "Epoch 165/500\n",
      "120/120 [==============================] - 0s 440us/sample - loss: 0.0991 - accuracy: 0.9500 - val_loss: 0.0259 - val_accuracy: 1.0000\n",
      "Epoch 166/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 378us/sample - loss: 0.0934 - accuracy: 0.9417 - val_loss: 0.0264 - val_accuracy: 1.0000\n",
      "Epoch 167/500\n",
      "120/120 [==============================] - 0s 415us/sample - loss: 0.0898 - accuracy: 0.9500 - val_loss: 0.0301 - val_accuracy: 1.0000\n",
      "Epoch 168/500\n",
      "120/120 [==============================] - 0s 432us/sample - loss: 0.0954 - accuracy: 0.9417 - val_loss: 0.0281 - val_accuracy: 1.0000\n",
      "Epoch 169/500\n",
      "120/120 [==============================] - 0s 382us/sample - loss: 0.0958 - accuracy: 0.9500 - val_loss: 0.0262 - val_accuracy: 1.0000\n",
      "Epoch 170/500\n",
      "120/120 [==============================] - 0s 391us/sample - loss: 0.0962 - accuracy: 0.9500 - val_loss: 0.0261 - val_accuracy: 1.0000\n",
      "Epoch 171/500\n",
      "120/120 [==============================] - 0s 432us/sample - loss: 0.0961 - accuracy: 0.9417 - val_loss: 0.0264 - val_accuracy: 1.0000\n",
      "Epoch 172/500\n",
      "120/120 [==============================] - 0s 388us/sample - loss: 0.0965 - accuracy: 0.9417 - val_loss: 0.0258 - val_accuracy: 1.0000\n",
      "Epoch 173/500\n",
      "120/120 [==============================] - 0s 383us/sample - loss: 0.0958 - accuracy: 0.9417 - val_loss: 0.0262 - val_accuracy: 1.0000\n",
      "Epoch 174/500\n",
      "120/120 [==============================] - 0s 415us/sample - loss: 0.1022 - accuracy: 0.9500 - val_loss: 0.0271 - val_accuracy: 1.0000\n",
      "Epoch 175/500\n",
      "120/120 [==============================] - 0s 432us/sample - loss: 0.0970 - accuracy: 0.9500 - val_loss: 0.0260 - val_accuracy: 1.0000\n",
      "Epoch 176/500\n",
      "120/120 [==============================] - 0s 509us/sample - loss: 0.0972 - accuracy: 0.9417 - val_loss: 0.0309 - val_accuracy: 1.0000\n",
      "Epoch 177/500\n",
      "120/120 [==============================] - 0s 449us/sample - loss: 0.0966 - accuracy: 0.9500 - val_loss: 0.0275 - val_accuracy: 1.0000\n",
      "Epoch 178/500\n",
      "120/120 [==============================] - 0s 407us/sample - loss: 0.0963 - accuracy: 0.9417 - val_loss: 0.0258 - val_accuracy: 1.0000\n",
      "Epoch 179/500\n",
      "120/120 [==============================] - 0s 382us/sample - loss: 0.0943 - accuracy: 0.9417 - val_loss: 0.0263 - val_accuracy: 1.0000\n",
      "Epoch 180/500\n",
      "120/120 [==============================] - 0s 424us/sample - loss: 0.0917 - accuracy: 0.9417 - val_loss: 0.0309 - val_accuracy: 1.0000\n",
      "Epoch 181/500\n",
      "120/120 [==============================] - 0s 416us/sample - loss: 0.0958 - accuracy: 0.9583 - val_loss: 0.0257 - val_accuracy: 1.0000\n",
      "Epoch 182/500\n",
      "120/120 [==============================] - 0s 358us/sample - loss: 0.0946 - accuracy: 0.9417 - val_loss: 0.0267 - val_accuracy: 1.0000\n",
      "Epoch 183/500\n",
      "120/120 [==============================] - 0s 366us/sample - loss: 0.1004 - accuracy: 0.9417 - val_loss: 0.0260 - val_accuracy: 1.0000\n",
      "Epoch 184/500\n",
      "120/120 [==============================] - 0s 352us/sample - loss: 0.0928 - accuracy: 0.9417 - val_loss: 0.0262 - val_accuracy: 1.0000\n",
      "Epoch 185/500\n",
      "120/120 [==============================] - 0s 364us/sample - loss: 0.0938 - accuracy: 0.9417 - val_loss: 0.0329 - val_accuracy: 1.0000\n",
      "Epoch 186/500\n",
      "120/120 [==============================] - 0s 391us/sample - loss: 0.0991 - accuracy: 0.9667 - val_loss: 0.0266 - val_accuracy: 1.0000\n",
      "Epoch 187/500\n",
      "120/120 [==============================] - 0s 374us/sample - loss: 0.0977 - accuracy: 0.9500 - val_loss: 0.0269 - val_accuracy: 1.0000\n",
      "Epoch 188/500\n",
      "120/120 [==============================] - 0s 433us/sample - loss: 0.0965 - accuracy: 0.9417 - val_loss: 0.0258 - val_accuracy: 1.0000\n",
      "Epoch 189/500\n",
      "120/120 [==============================] - 0s 366us/sample - loss: 0.0955 - accuracy: 0.9417 - val_loss: 0.0257 - val_accuracy: 1.0000\n",
      "Epoch 190/500\n",
      "120/120 [==============================] - 0s 370us/sample - loss: 0.0930 - accuracy: 0.9417 - val_loss: 0.0258 - val_accuracy: 1.0000\n",
      "Epoch 191/500\n",
      "120/120 [==============================] - 0s 429us/sample - loss: 0.0946 - accuracy: 0.9500 - val_loss: 0.0267 - val_accuracy: 1.0000\n",
      "Epoch 192/500\n",
      "120/120 [==============================] - 0s 357us/sample - loss: 0.0980 - accuracy: 0.9500 - val_loss: 0.0265 - val_accuracy: 1.0000\n",
      "Epoch 193/500\n",
      "120/120 [==============================] - 0s 391us/sample - loss: 0.0949 - accuracy: 0.9417 - val_loss: 0.0275 - val_accuracy: 1.0000\n",
      "Epoch 194/500\n",
      "120/120 [==============================] - 0s 357us/sample - loss: 0.0984 - accuracy: 0.9500 - val_loss: 0.0291 - val_accuracy: 1.0000\n",
      "Epoch 195/500\n",
      "120/120 [==============================] - 0s 374us/sample - loss: 0.0933 - accuracy: 0.9500 - val_loss: 0.0274 - val_accuracy: 1.0000\n",
      "Epoch 196/500\n",
      "120/120 [==============================] - 0s 382us/sample - loss: 0.0966 - accuracy: 0.9417 - val_loss: 0.0274 - val_accuracy: 1.0000\n",
      "Epoch 197/500\n",
      "120/120 [==============================] - 0s 352us/sample - loss: 0.0960 - accuracy: 0.9417 - val_loss: 0.0256 - val_accuracy: 1.0000\n",
      "Epoch 198/500\n",
      "120/120 [==============================] - 0s 364us/sample - loss: 0.0979 - accuracy: 0.9417 - val_loss: 0.0274 - val_accuracy: 1.0000\n",
      "Epoch 199/500\n",
      "120/120 [==============================] - 0s 474us/sample - loss: 0.0964 - accuracy: 0.9500 - val_loss: 0.0256 - val_accuracy: 1.0000\n",
      "Epoch 200/500\n",
      "120/120 [==============================] - 0s 499us/sample - loss: 0.0952 - accuracy: 0.9417 - val_loss: 0.0258 - val_accuracy: 1.0000\n",
      "Epoch 201/500\n",
      "120/120 [==============================] - 0s 482us/sample - loss: 0.0933 - accuracy: 0.9500 - val_loss: 0.0262 - val_accuracy: 1.0000\n",
      "Epoch 202/500\n",
      "120/120 [==============================] - 0s 399us/sample - loss: 0.0885 - accuracy: 0.9500 - val_loss: 0.0292 - val_accuracy: 1.0000\n",
      "Epoch 203/500\n",
      "120/120 [==============================] - 0s 457us/sample - loss: 0.0959 - accuracy: 0.9500 - val_loss: 0.0256 - val_accuracy: 1.0000\n",
      "Epoch 204/500\n",
      "120/120 [==============================] - 0s 607us/sample - loss: 0.0943 - accuracy: 0.9417 - val_loss: 0.0270 - val_accuracy: 1.0000\n",
      "Epoch 205/500\n",
      "120/120 [==============================] - 0s 365us/sample - loss: 0.1008 - accuracy: 0.9417 - val_loss: 0.0255 - val_accuracy: 1.0000\n",
      "Epoch 206/500\n",
      "120/120 [==============================] - 0s 425us/sample - loss: 0.0968 - accuracy: 0.9417 - val_loss: 0.0263 - val_accuracy: 1.0000\n",
      "Epoch 207/500\n",
      "120/120 [==============================] - 0s 372us/sample - loss: 0.0982 - accuracy: 0.9417 - val_loss: 0.0254 - val_accuracy: 1.0000\n",
      "Epoch 208/500\n",
      "120/120 [==============================] - 0s 357us/sample - loss: 0.0918 - accuracy: 0.9667 - val_loss: 0.0277 - val_accuracy: 1.0000\n",
      "Epoch 209/500\n",
      "120/120 [==============================] - 0s 368us/sample - loss: 0.0946 - accuracy: 0.9417 - val_loss: 0.0266 - val_accuracy: 1.0000\n",
      "Epoch 210/500\n",
      "120/120 [==============================] - 0s 378us/sample - loss: 0.0962 - accuracy: 0.9417 - val_loss: 0.0254 - val_accuracy: 1.0000\n",
      "Epoch 211/500\n",
      "120/120 [==============================] - 0s 419us/sample - loss: 0.0988 - accuracy: 0.9417 - val_loss: 0.0255 - val_accuracy: 1.0000\n",
      "Epoch 212/500\n",
      "120/120 [==============================] - 0s 366us/sample - loss: 0.0933 - accuracy: 0.9417 - val_loss: 0.0254 - val_accuracy: 1.0000\n",
      "Epoch 213/500\n",
      "120/120 [==============================] - 0s 366us/sample - loss: 0.0891 - accuracy: 0.9583 - val_loss: 0.0279 - val_accuracy: 1.0000\n",
      "Epoch 214/500\n",
      "120/120 [==============================] - 0s 354us/sample - loss: 0.0975 - accuracy: 0.9417 - val_loss: 0.0254 - val_accuracy: 1.0000\n",
      "Epoch 215/500\n",
      "120/120 [==============================] - 0s 357us/sample - loss: 0.0971 - accuracy: 0.9417 - val_loss: 0.0257 - val_accuracy: 1.0000\n",
      "Epoch 216/500\n",
      "120/120 [==============================] - 0s 422us/sample - loss: 0.0921 - accuracy: 0.9500 - val_loss: 0.0290 - val_accuracy: 1.0000\n",
      "Epoch 217/500\n",
      "120/120 [==============================] - 0s 391us/sample - loss: 0.0978 - accuracy: 0.9500 - val_loss: 0.0255 - val_accuracy: 1.0000\n",
      "Epoch 218/500\n",
      "120/120 [==============================] - 0s 386us/sample - loss: 0.0934 - accuracy: 0.9417 - val_loss: 0.0260 - val_accuracy: 1.0000\n",
      "Epoch 219/500\n",
      "120/120 [==============================] - 0s 374us/sample - loss: 0.0943 - accuracy: 0.9583 - val_loss: 0.0278 - val_accuracy: 1.0000\n",
      "Epoch 220/500\n",
      "120/120 [==============================] - 0s 350us/sample - loss: 0.0945 - accuracy: 0.9583 - val_loss: 0.0265 - val_accuracy: 1.0000\n",
      "Epoch 221/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 357us/sample - loss: 0.0975 - accuracy: 0.9417 - val_loss: 0.0263 - val_accuracy: 1.0000\n",
      "Epoch 222/500\n",
      "120/120 [==============================] - 0s 349us/sample - loss: 0.0977 - accuracy: 0.9500 - val_loss: 0.0258 - val_accuracy: 1.0000\n",
      "Epoch 223/500\n",
      "120/120 [==============================] - 0s 366us/sample - loss: 0.0961 - accuracy: 0.9417 - val_loss: 0.0253 - val_accuracy: 1.0000\n",
      "Epoch 224/500\n",
      "120/120 [==============================] - 0s 374us/sample - loss: 0.0925 - accuracy: 0.9417 - val_loss: 0.0288 - val_accuracy: 1.0000\n",
      "Epoch 225/500\n",
      "120/120 [==============================] - 0s 357us/sample - loss: 0.0919 - accuracy: 0.9500 - val_loss: 0.0259 - val_accuracy: 1.0000\n",
      "Epoch 226/500\n",
      "120/120 [==============================] - 0s 366us/sample - loss: 0.0959 - accuracy: 0.9417 - val_loss: 0.0256 - val_accuracy: 1.0000\n",
      "Epoch 227/500\n",
      "120/120 [==============================] - 0s 411us/sample - loss: 0.0945 - accuracy: 0.9500 - val_loss: 0.0253 - val_accuracy: 1.0000\n",
      "Epoch 228/500\n",
      "120/120 [==============================] - 0s 374us/sample - loss: 0.0954 - accuracy: 0.9417 - val_loss: 0.0253 - val_accuracy: 1.0000\n",
      "Epoch 229/500\n",
      "120/120 [==============================] - 0s 391us/sample - loss: 0.0943 - accuracy: 0.9417 - val_loss: 0.0253 - val_accuracy: 1.0000\n",
      "Epoch 230/500\n",
      "120/120 [==============================] - 0s 366us/sample - loss: 0.0916 - accuracy: 0.9583 - val_loss: 0.0290 - val_accuracy: 1.0000\n",
      "Epoch 231/500\n",
      "120/120 [==============================] - 0s 400us/sample - loss: 0.1007 - accuracy: 0.9417 - val_loss: 0.0256 - val_accuracy: 1.0000\n",
      "Epoch 232/500\n",
      "120/120 [==============================] - 0s 391us/sample - loss: 0.0971 - accuracy: 0.9417 - val_loss: 0.0252 - val_accuracy: 1.0000\n",
      "Epoch 233/500\n",
      "120/120 [==============================] - 0s 366us/sample - loss: 0.0929 - accuracy: 0.9500 - val_loss: 0.0252 - val_accuracy: 1.0000\n",
      "Epoch 234/500\n",
      "120/120 [==============================] - 0s 381us/sample - loss: 0.0967 - accuracy: 0.9500 - val_loss: 0.0256 - val_accuracy: 1.0000\n",
      "Epoch 235/500\n",
      "120/120 [==============================] - 0s 432us/sample - loss: 0.0943 - accuracy: 0.9417 - val_loss: 0.0254 - val_accuracy: 1.0000\n",
      "Epoch 236/500\n",
      "120/120 [==============================] - 0s 374us/sample - loss: 0.0972 - accuracy: 0.9417 - val_loss: 0.0256 - val_accuracy: 1.0000\n",
      "Epoch 237/500\n",
      "120/120 [==============================] - 0s 382us/sample - loss: 0.0936 - accuracy: 0.9417 - val_loss: 0.0342 - val_accuracy: 1.0000\n",
      "Epoch 238/500\n",
      "120/120 [==============================] - 0s 357us/sample - loss: 0.0959 - accuracy: 0.9667 - val_loss: 0.0255 - val_accuracy: 1.0000\n",
      "Epoch 239/500\n",
      "120/120 [==============================] - 0s 392us/sample - loss: 0.0943 - accuracy: 0.9417 - val_loss: 0.0252 - val_accuracy: 1.0000\n",
      "Epoch 240/500\n",
      "120/120 [==============================] - 0s 357us/sample - loss: 0.0975 - accuracy: 0.9417 - val_loss: 0.0254 - val_accuracy: 1.0000\n",
      "Epoch 241/500\n",
      "120/120 [==============================] - 0s 374us/sample - loss: 0.0976 - accuracy: 0.9417 - val_loss: 0.0252 - val_accuracy: 1.0000\n",
      "Epoch 242/500\n",
      "120/120 [==============================] - 0s 400us/sample - loss: 0.0981 - accuracy: 0.9417 - val_loss: 0.0254 - val_accuracy: 1.0000\n",
      "Epoch 243/500\n",
      "120/120 [==============================] - 0s 457us/sample - loss: 0.0937 - accuracy: 0.9500 - val_loss: 0.0262 - val_accuracy: 1.0000\n",
      "Epoch 244/500\n",
      "120/120 [==============================] - 0s 373us/sample - loss: 0.0937 - accuracy: 0.9500 - val_loss: 0.0259 - val_accuracy: 1.0000\n",
      "Epoch 245/500\n",
      "120/120 [==============================] - 0s 374us/sample - loss: 0.0950 - accuracy: 0.9500 - val_loss: 0.0253 - val_accuracy: 1.0000\n",
      "Epoch 246/500\n",
      "120/120 [==============================] - 0s 415us/sample - loss: 0.0972 - accuracy: 0.9500 - val_loss: 0.0251 - val_accuracy: 1.0000\n",
      "Epoch 247/500\n",
      "120/120 [==============================] - 0s 374us/sample - loss: 0.0951 - accuracy: 0.9417 - val_loss: 0.0255 - val_accuracy: 1.0000\n",
      "Epoch 248/500\n",
      "120/120 [==============================] - 0s 374us/sample - loss: 0.0952 - accuracy: 0.9417 - val_loss: 0.0273 - val_accuracy: 1.0000\n",
      "Epoch 249/500\n",
      "120/120 [==============================] - 0s 357us/sample - loss: 0.0952 - accuracy: 0.9417 - val_loss: 0.0251 - val_accuracy: 1.0000\n",
      "Epoch 250/500\n",
      "120/120 [==============================] - 0s 374us/sample - loss: 0.0964 - accuracy: 0.9417 - val_loss: 0.0291 - val_accuracy: 1.0000\n",
      "Epoch 251/500\n",
      "120/120 [==============================] - 0s 366us/sample - loss: 0.0953 - accuracy: 0.9500 - val_loss: 0.0259 - val_accuracy: 1.0000\n",
      "Epoch 252/500\n",
      "120/120 [==============================] - 0s 360us/sample - loss: 0.0962 - accuracy: 0.9417 - val_loss: 0.0254 - val_accuracy: 1.0000\n",
      "Epoch 253/500\n",
      "120/120 [==============================] - 0s 428us/sample - loss: 0.0956 - accuracy: 0.9417 - val_loss: 0.0254 - val_accuracy: 1.0000\n",
      "Epoch 254/500\n",
      "120/120 [==============================] - 0s 382us/sample - loss: 0.0997 - accuracy: 0.9417 - val_loss: 0.0255 - val_accuracy: 1.0000\n",
      "Epoch 255/500\n",
      "120/120 [==============================] - 0s 391us/sample - loss: 0.0928 - accuracy: 0.9500 - val_loss: 0.0273 - val_accuracy: 1.0000\n",
      "Epoch 256/500\n",
      "120/120 [==============================] - 0s 524us/sample - loss: 0.0955 - accuracy: 0.9583 - val_loss: 0.0250 - val_accuracy: 1.0000\n",
      "Epoch 257/500\n",
      "120/120 [==============================] - 0s 457us/sample - loss: 0.0946 - accuracy: 0.9417 - val_loss: 0.0257 - val_accuracy: 1.0000\n",
      "Epoch 258/500\n",
      "120/120 [==============================] - 0s 399us/sample - loss: 0.0939 - accuracy: 0.9417 - val_loss: 0.0271 - val_accuracy: 1.0000\n",
      "Epoch 259/500\n",
      "120/120 [==============================] - 0s 366us/sample - loss: 0.0915 - accuracy: 0.9583 - val_loss: 0.0278 - val_accuracy: 1.0000\n",
      "Epoch 260/500\n",
      "120/120 [==============================] - 0s 399us/sample - loss: 0.0946 - accuracy: 0.9417 - val_loss: 0.0271 - val_accuracy: 1.0000\n",
      "Epoch 261/500\n",
      "120/120 [==============================] - 0s 357us/sample - loss: 0.0944 - accuracy: 0.9417 - val_loss: 0.0260 - val_accuracy: 1.0000\n",
      "Epoch 262/500\n",
      "120/120 [==============================] - 0s 356us/sample - loss: 0.0982 - accuracy: 0.9417 - val_loss: 0.0262 - val_accuracy: 1.0000\n",
      "Epoch 263/500\n",
      "120/120 [==============================] - 0s 423us/sample - loss: 0.1018 - accuracy: 0.9417 - val_loss: 0.0250 - val_accuracy: 1.0000\n",
      "Epoch 264/500\n",
      "120/120 [==============================] - 0s 440us/sample - loss: 0.0927 - accuracy: 0.9417 - val_loss: 0.0250 - val_accuracy: 1.0000\n",
      "Epoch 265/500\n",
      "120/120 [==============================] - 0s 484us/sample - loss: 0.0920 - accuracy: 0.9417 - val_loss: 0.0265 - val_accuracy: 1.0000\n",
      "Epoch 266/500\n",
      "120/120 [==============================] - 0s 366us/sample - loss: 0.0952 - accuracy: 0.9500 - val_loss: 0.0256 - val_accuracy: 1.0000\n",
      "Epoch 267/500\n",
      "120/120 [==============================] - 0s 490us/sample - loss: 0.0964 - accuracy: 0.9500 - val_loss: 0.0251 - val_accuracy: 1.0000\n",
      "Epoch 268/500\n",
      "120/120 [==============================] - 0s 401us/sample - loss: 0.0969 - accuracy: 0.9417 - val_loss: 0.0256 - val_accuracy: 1.0000\n",
      "Epoch 269/500\n",
      "120/120 [==============================] - 0s 366us/sample - loss: 0.0935 - accuracy: 0.9417 - val_loss: 0.0278 - val_accuracy: 1.0000\n",
      "Epoch 270/500\n",
      "120/120 [==============================] - 0s 439us/sample - loss: 0.0963 - accuracy: 0.9500 - val_loss: 0.0275 - val_accuracy: 1.0000\n",
      "Epoch 271/500\n",
      "120/120 [==============================] - 0s 341us/sample - loss: 0.0961 - accuracy: 0.9417 - val_loss: 0.0250 - val_accuracy: 1.0000\n",
      "Epoch 272/500\n",
      "120/120 [==============================] - 0s 382us/sample - loss: 0.0929 - accuracy: 0.9500 - val_loss: 0.0261 - val_accuracy: 1.0000\n",
      "Epoch 273/500\n",
      "120/120 [==============================] - 0s 407us/sample - loss: 0.0981 - accuracy: 0.9417 - val_loss: 0.0250 - val_accuracy: 1.0000\n",
      "Epoch 274/500\n",
      "120/120 [==============================] - 0s 366us/sample - loss: 0.0936 - accuracy: 0.9417 - val_loss: 0.0250 - val_accuracy: 1.0000\n",
      "Epoch 275/500\n",
      "120/120 [==============================] - 0s 374us/sample - loss: 0.0970 - accuracy: 0.9417 - val_loss: 0.0249 - val_accuracy: 1.0000\n",
      "Epoch 276/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 374us/sample - loss: 0.0972 - accuracy: 0.9417 - val_loss: 0.0250 - val_accuracy: 1.0000\n",
      "Epoch 277/500\n",
      "120/120 [==============================] - 0s 399us/sample - loss: 0.0980 - accuracy: 0.9417 - val_loss: 0.0253 - val_accuracy: 1.0000\n",
      "Epoch 278/500\n",
      "120/120 [==============================] - 0s 399us/sample - loss: 0.0949 - accuracy: 0.9417 - val_loss: 0.0264 - val_accuracy: 1.0000\n",
      "Epoch 279/500\n",
      "120/120 [==============================] - 0s 382us/sample - loss: 0.0949 - accuracy: 0.9500 - val_loss: 0.0251 - val_accuracy: 1.0000\n",
      "Epoch 280/500\n",
      "120/120 [==============================] - 0s 374us/sample - loss: 0.0958 - accuracy: 0.9417 - val_loss: 0.0249 - val_accuracy: 1.0000\n",
      "Epoch 281/500\n",
      "120/120 [==============================] - 0s 366us/sample - loss: 0.0958 - accuracy: 0.9417 - val_loss: 0.0253 - val_accuracy: 1.0000\n",
      "Epoch 282/500\n",
      "120/120 [==============================] - 0s 432us/sample - loss: 0.0951 - accuracy: 0.9500 - val_loss: 0.0264 - val_accuracy: 1.0000\n",
      "Epoch 283/500\n",
      "120/120 [==============================] - 0s 374us/sample - loss: 0.0948 - accuracy: 0.9417 - val_loss: 0.0253 - val_accuracy: 1.0000\n",
      "Epoch 284/500\n",
      "120/120 [==============================] - 0s 441us/sample - loss: 0.0949 - accuracy: 0.9500 - val_loss: 0.0249 - val_accuracy: 1.0000\n",
      "Epoch 285/500\n",
      "120/120 [==============================] - 0s 415us/sample - loss: 0.0945 - accuracy: 0.9500 - val_loss: 0.0269 - val_accuracy: 1.0000\n",
      "Epoch 286/500\n",
      "120/120 [==============================] - 0s 357us/sample - loss: 0.0973 - accuracy: 0.9417 - val_loss: 0.0265 - val_accuracy: 1.0000\n",
      "Epoch 287/500\n",
      "120/120 [==============================] - 0s 477us/sample - loss: 0.0982 - accuracy: 0.9417 - val_loss: 0.0248 - val_accuracy: 1.0000\n",
      "Epoch 288/500\n",
      "120/120 [==============================] - 0s 374us/sample - loss: 0.0934 - accuracy: 0.9417 - val_loss: 0.0249 - val_accuracy: 1.0000\n",
      "Epoch 289/500\n",
      "120/120 [==============================] - 0s 365us/sample - loss: 0.0921 - accuracy: 0.9417 - val_loss: 0.0261 - val_accuracy: 1.0000\n",
      "Epoch 290/500\n",
      "120/120 [==============================] - 0s 374us/sample - loss: 0.0990 - accuracy: 0.9417 - val_loss: 0.0248 - val_accuracy: 1.0000\n",
      "Epoch 291/500\n",
      "120/120 [==============================] - 0s 385us/sample - loss: 0.0913 - accuracy: 0.9500 - val_loss: 0.0251 - val_accuracy: 1.0000\n",
      "Epoch 292/500\n",
      "120/120 [==============================] - 0s 382us/sample - loss: 0.0914 - accuracy: 0.9500 - val_loss: 0.0339 - val_accuracy: 1.0000\n",
      "Epoch 293/500\n",
      "120/120 [==============================] - 0s 415us/sample - loss: 0.0974 - accuracy: 0.9583 - val_loss: 0.0254 - val_accuracy: 1.0000\n",
      "Epoch 294/500\n",
      "120/120 [==============================] - 0s 366us/sample - loss: 0.0957 - accuracy: 0.9417 - val_loss: 0.0248 - val_accuracy: 1.0000\n",
      "Epoch 295/500\n",
      "120/120 [==============================] - 0s 366us/sample - loss: 0.0989 - accuracy: 0.9417 - val_loss: 0.0248 - val_accuracy: 1.0000\n",
      "Epoch 296/500\n",
      "120/120 [==============================] - 0s 366us/sample - loss: 0.0954 - accuracy: 0.9417 - val_loss: 0.0279 - val_accuracy: 1.0000\n",
      "Epoch 297/500\n",
      "120/120 [==============================] - 0s 357us/sample - loss: 0.0928 - accuracy: 0.9500 - val_loss: 0.0251 - val_accuracy: 1.0000\n",
      "Epoch 298/500\n",
      "120/120 [==============================] - 0s 374us/sample - loss: 0.0951 - accuracy: 0.9417 - val_loss: 0.0255 - val_accuracy: 1.0000\n",
      "Epoch 299/500\n",
      "120/120 [==============================] - 0s 357us/sample - loss: 0.0953 - accuracy: 0.9417 - val_loss: 0.0248 - val_accuracy: 1.0000\n",
      "Epoch 300/500\n",
      "120/120 [==============================] - 0s 382us/sample - loss: 0.0944 - accuracy: 0.9417 - val_loss: 0.0251 - val_accuracy: 1.0000\n",
      "Epoch 301/500\n",
      "120/120 [==============================] - 0s 363us/sample - loss: 0.0930 - accuracy: 0.9417 - val_loss: 0.0248 - val_accuracy: 1.0000\n",
      "Epoch 302/500\n",
      "120/120 [==============================] - 0s 557us/sample - loss: 0.0990 - accuracy: 0.9417 - val_loss: 0.0248 - val_accuracy: 1.0000\n",
      "Epoch 303/500\n",
      "120/120 [==============================] - 0s 382us/sample - loss: 0.0930 - accuracy: 0.9417 - val_loss: 0.0258 - val_accuracy: 1.0000\n",
      "Epoch 304/500\n",
      "120/120 [==============================] - 0s 423us/sample - loss: 0.0981 - accuracy: 0.9500 - val_loss: 0.0248 - val_accuracy: 1.0000\n",
      "Epoch 305/500\n",
      "120/120 [==============================] - 0s 357us/sample - loss: 0.0961 - accuracy: 0.9500 - val_loss: 0.0257 - val_accuracy: 1.0000\n",
      "Epoch 306/500\n",
      "120/120 [==============================] - 0s 381us/sample - loss: 0.0884 - accuracy: 0.9583 - val_loss: 0.0299 - val_accuracy: 1.0000\n",
      "Epoch 307/500\n",
      "120/120 [==============================] - 0s 366us/sample - loss: 0.0998 - accuracy: 0.9500 - val_loss: 0.0248 - val_accuracy: 1.0000\n",
      "Epoch 308/500\n",
      "120/120 [==============================] - 0s 416us/sample - loss: 0.0940 - accuracy: 0.9417 - val_loss: 0.0272 - val_accuracy: 1.0000\n",
      "Epoch 309/500\n",
      "120/120 [==============================] - 0s 366us/sample - loss: 0.0962 - accuracy: 0.9583 - val_loss: 0.0247 - val_accuracy: 1.0000\n",
      "Epoch 310/500\n",
      "120/120 [==============================] - 0s 406us/sample - loss: 0.0962 - accuracy: 0.9500 - val_loss: 0.0252 - val_accuracy: 1.0000\n",
      "Epoch 311/500\n",
      "120/120 [==============================] - 0s 399us/sample - loss: 0.0962 - accuracy: 0.9500 - val_loss: 0.0247 - val_accuracy: 1.0000\n",
      "Epoch 312/500\n",
      "120/120 [==============================] - 0s 402us/sample - loss: 0.0924 - accuracy: 0.9500 - val_loss: 0.0262 - val_accuracy: 1.0000\n",
      "Epoch 313/500\n",
      "120/120 [==============================] - 0s 366us/sample - loss: 0.0945 - accuracy: 0.9500 - val_loss: 0.0256 - val_accuracy: 1.0000\n",
      "Epoch 314/500\n",
      "120/120 [==============================] - 0s 421us/sample - loss: 0.0939 - accuracy: 0.9583 - val_loss: 0.0251 - val_accuracy: 1.0000\n",
      "Epoch 315/500\n",
      "120/120 [==============================] - 0s 358us/sample - loss: 0.0906 - accuracy: 0.9417 - val_loss: 0.0262 - val_accuracy: 1.0000\n",
      "Epoch 316/500\n",
      "120/120 [==============================] - 0s 361us/sample - loss: 0.0968 - accuracy: 0.9500 - val_loss: 0.0296 - val_accuracy: 1.0000\n",
      "Epoch 317/500\n",
      "120/120 [==============================] - 0s 357us/sample - loss: 0.0966 - accuracy: 0.9417 - val_loss: 0.0284 - val_accuracy: 1.0000\n",
      "Epoch 318/500\n",
      "120/120 [==============================] - 0s 432us/sample - loss: 0.0949 - accuracy: 0.9583 - val_loss: 0.0247 - val_accuracy: 1.0000\n",
      "Epoch 319/500\n",
      "120/120 [==============================] - 0s 382us/sample - loss: 0.0948 - accuracy: 0.9417 - val_loss: 0.0247 - val_accuracy: 1.0000\n",
      "Epoch 320/500\n",
      "120/120 [==============================] - 0s 366us/sample - loss: 0.0910 - accuracy: 0.9417 - val_loss: 0.0266 - val_accuracy: 1.0000\n",
      "Epoch 321/500\n",
      "120/120 [==============================] - 0s 349us/sample - loss: 0.0947 - accuracy: 0.9500 - val_loss: 0.0248 - val_accuracy: 1.0000\n",
      "Epoch 322/500\n",
      "120/120 [==============================] - 0s 359us/sample - loss: 0.0918 - accuracy: 0.9417 - val_loss: 0.0251 - val_accuracy: 1.0000\n",
      "Epoch 323/500\n",
      "120/120 [==============================] - 0s 388us/sample - loss: 0.0971 - accuracy: 0.9500 - val_loss: 0.0246 - val_accuracy: 1.0000\n",
      "Epoch 324/500\n",
      "120/120 [==============================] - 0s 366us/sample - loss: 0.0966 - accuracy: 0.9500 - val_loss: 0.0250 - val_accuracy: 1.0000\n",
      "Epoch 325/500\n",
      "120/120 [==============================] - 0s 364us/sample - loss: 0.0936 - accuracy: 0.9417 - val_loss: 0.0246 - val_accuracy: 1.0000\n",
      "Epoch 326/500\n",
      "120/120 [==============================] - 0s 378us/sample - loss: 0.0957 - accuracy: 0.9417 - val_loss: 0.0252 - val_accuracy: 1.0000\n",
      "Epoch 327/500\n",
      "120/120 [==============================] - 0s 373us/sample - loss: 0.1036 - accuracy: 0.9583 - val_loss: 0.0246 - val_accuracy: 1.0000\n",
      "Epoch 328/500\n",
      "120/120 [==============================] - 0s 482us/sample - loss: 0.0888 - accuracy: 0.9417 - val_loss: 0.0275 - val_accuracy: 1.0000\n",
      "Epoch 329/500\n",
      "120/120 [==============================] - 0s 712us/sample - loss: 0.0945 - accuracy: 0.9583 - val_loss: 0.0252 - val_accuracy: 1.0000\n",
      "Epoch 330/500\n",
      "120/120 [==============================] - 0s 449us/sample - loss: 0.0938 - accuracy: 0.9417 - val_loss: 0.0250 - val_accuracy: 1.0000\n",
      "Epoch 331/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 391us/sample - loss: 0.0940 - accuracy: 0.9417 - val_loss: 0.0246 - val_accuracy: 1.0000\n",
      "Epoch 332/500\n",
      "120/120 [==============================] - 0s 449us/sample - loss: 0.0976 - accuracy: 0.9417 - val_loss: 0.0246 - val_accuracy: 1.0000\n",
      "Epoch 333/500\n",
      "120/120 [==============================] - 0s 382us/sample - loss: 0.0971 - accuracy: 0.9417 - val_loss: 0.0245 - val_accuracy: 1.0000\n",
      "Epoch 334/500\n",
      "120/120 [==============================] - 0s 374us/sample - loss: 0.0918 - accuracy: 0.9417 - val_loss: 0.0259 - val_accuracy: 1.0000\n",
      "Epoch 335/500\n",
      "120/120 [==============================] - 0s 413us/sample - loss: 0.0865 - accuracy: 0.9583 - val_loss: 0.0295 - val_accuracy: 1.0000\n",
      "Epoch 336/500\n",
      "120/120 [==============================] - 0s 457us/sample - loss: 0.0998 - accuracy: 0.9500 - val_loss: 0.0245 - val_accuracy: 1.0000\n",
      "Epoch 337/500\n",
      "120/120 [==============================] - 0s 490us/sample - loss: 0.0978 - accuracy: 0.9417 - val_loss: 0.0250 - val_accuracy: 1.0000\n",
      "Epoch 338/500\n",
      "120/120 [==============================] - 0s 416us/sample - loss: 0.1007 - accuracy: 0.9417 - val_loss: 0.0262 - val_accuracy: 1.0000\n",
      "Epoch 339/500\n",
      "120/120 [==============================] - 0s 532us/sample - loss: 0.0947 - accuracy: 0.9417 - val_loss: 0.0246 - val_accuracy: 1.0000\n",
      "Epoch 340/500\n",
      "120/120 [==============================] - 0s 416us/sample - loss: 0.0947 - accuracy: 0.9500 - val_loss: 0.0245 - val_accuracy: 1.0000\n",
      "Epoch 341/500\n",
      "120/120 [==============================] - 0s 391us/sample - loss: 0.0983 - accuracy: 0.9417 - val_loss: 0.0256 - val_accuracy: 1.0000\n",
      "Epoch 342/500\n",
      "120/120 [==============================] - 0s 390us/sample - loss: 0.0973 - accuracy: 0.9417 - val_loss: 0.0281 - val_accuracy: 1.0000\n",
      "Epoch 343/500\n",
      "120/120 [==============================] - 0s 424us/sample - loss: 0.0936 - accuracy: 0.9583 - val_loss: 0.0247 - val_accuracy: 1.0000\n",
      "Epoch 344/500\n",
      "120/120 [==============================] - 0s 391us/sample - loss: 0.0923 - accuracy: 0.9417 - val_loss: 0.0250 - val_accuracy: 1.0000\n",
      "Epoch 345/500\n",
      "120/120 [==============================] - 0s 407us/sample - loss: 0.0975 - accuracy: 0.9417 - val_loss: 0.0248 - val_accuracy: 1.0000\n",
      "Epoch 346/500\n",
      "120/120 [==============================] - 0s 474us/sample - loss: 0.0942 - accuracy: 0.9417 - val_loss: 0.0246 - val_accuracy: 1.0000\n",
      "Epoch 347/500\n",
      "120/120 [==============================] - 0s 424us/sample - loss: 0.0949 - accuracy: 0.9500 - val_loss: 0.0248 - val_accuracy: 1.0000\n",
      "Epoch 348/500\n",
      "120/120 [==============================] - 0s 371us/sample - loss: 0.0950 - accuracy: 0.9417 - val_loss: 0.0294 - val_accuracy: 1.0000\n",
      "Epoch 349/500\n",
      "120/120 [==============================] - 0s 582us/sample - loss: 0.0953 - accuracy: 0.9417 - val_loss: 0.0299 - val_accuracy: 1.0000\n",
      "Epoch 350/500\n",
      "120/120 [==============================] - 0s 399us/sample - loss: 0.0846 - accuracy: 0.9583 - val_loss: 0.0278 - val_accuracy: 1.0000\n",
      "Epoch 351/500\n",
      "120/120 [==============================] - 0s 411us/sample - loss: 0.0982 - accuracy: 0.9500 - val_loss: 0.0247 - val_accuracy: 1.0000\n",
      "Epoch 352/500\n",
      "120/120 [==============================] - 0s 424us/sample - loss: 0.0975 - accuracy: 0.9417 - val_loss: 0.0247 - val_accuracy: 1.0000\n",
      "Epoch 353/500\n",
      "120/120 [==============================] - 0s 399us/sample - loss: 0.0937 - accuracy: 0.9500 - val_loss: 0.0251 - val_accuracy: 1.0000\n",
      "Epoch 354/500\n",
      "120/120 [==============================] - 0s 416us/sample - loss: 0.0943 - accuracy: 0.9417 - val_loss: 0.0250 - val_accuracy: 1.0000\n",
      "Epoch 355/500\n",
      "120/120 [==============================] - 0s 382us/sample - loss: 0.0974 - accuracy: 0.9417 - val_loss: 0.0249 - val_accuracy: 1.0000\n",
      "Epoch 356/500\n",
      "120/120 [==============================] - 0s 383us/sample - loss: 0.0918 - accuracy: 0.9500 - val_loss: 0.0247 - val_accuracy: 1.0000\n",
      "Epoch 357/500\n",
      "120/120 [==============================] - 0s 390us/sample - loss: 0.0967 - accuracy: 0.9417 - val_loss: 0.0256 - val_accuracy: 1.0000\n",
      "Epoch 358/500\n",
      "120/120 [==============================] - 0s 447us/sample - loss: 0.0950 - accuracy: 0.9583 - val_loss: 0.0245 - val_accuracy: 1.0000\n",
      "Epoch 359/500\n",
      "120/120 [==============================] - 0s 407us/sample - loss: 0.0973 - accuracy: 0.9417 - val_loss: 0.0245 - val_accuracy: 1.0000\n",
      "Epoch 360/500\n",
      "120/120 [==============================] - 0s 380us/sample - loss: 0.0977 - accuracy: 0.9417 - val_loss: 0.0247 - val_accuracy: 1.0000\n",
      "Epoch 361/500\n",
      "120/120 [==============================] - 0s 366us/sample - loss: 0.0950 - accuracy: 0.9583 - val_loss: 0.0262 - val_accuracy: 1.0000\n",
      "Epoch 362/500\n",
      "120/120 [==============================] - 0s 382us/sample - loss: 0.0952 - accuracy: 0.9417 - val_loss: 0.0268 - val_accuracy: 1.0000\n",
      "Epoch 363/500\n",
      "120/120 [==============================] - 0s 374us/sample - loss: 0.0988 - accuracy: 0.9417 - val_loss: 0.0258 - val_accuracy: 1.0000\n",
      "Epoch 364/500\n",
      "120/120 [==============================] - 0s 382us/sample - loss: 0.1009 - accuracy: 0.9500 - val_loss: 0.0246 - val_accuracy: 1.0000\n",
      "Epoch 365/500\n",
      "120/120 [==============================] - 0s 349us/sample - loss: 0.0887 - accuracy: 0.9500 - val_loss: 0.0302 - val_accuracy: 1.0000\n",
      "Epoch 366/500\n",
      "120/120 [==============================] - 0s 366us/sample - loss: 0.0970 - accuracy: 0.9500 - val_loss: 0.0251 - val_accuracy: 1.0000\n",
      "Epoch 367/500\n",
      "120/120 [==============================] - 0s 366us/sample - loss: 0.0968 - accuracy: 0.9417 - val_loss: 0.0246 - val_accuracy: 1.0000\n",
      "Epoch 368/500\n",
      "120/120 [==============================] - 0s 423us/sample - loss: 0.1001 - accuracy: 0.9417 - val_loss: 0.0247 - val_accuracy: 1.0000\n",
      "Epoch 369/500\n",
      "120/120 [==============================] - 0s 424us/sample - loss: 0.0947 - accuracy: 0.9417 - val_loss: 0.0245 - val_accuracy: 1.0000\n",
      "Epoch 370/500\n",
      "120/120 [==============================] - 0s 366us/sample - loss: 0.0870 - accuracy: 0.9500 - val_loss: 0.0280 - val_accuracy: 1.0000\n",
      "Epoch 371/500\n",
      "120/120 [==============================] - 0s 399us/sample - loss: 0.0952 - accuracy: 0.9500 - val_loss: 0.0273 - val_accuracy: 1.0000\n",
      "Epoch 372/500\n",
      "120/120 [==============================] - 0s 449us/sample - loss: 0.0960 - accuracy: 0.9500 - val_loss: 0.0247 - val_accuracy: 1.0000\n",
      "Epoch 373/500\n",
      "120/120 [==============================] - 0s 417us/sample - loss: 0.0981 - accuracy: 0.9417 - val_loss: 0.0245 - val_accuracy: 1.0000\n",
      "Epoch 374/500\n",
      "120/120 [==============================] - 0s 357us/sample - loss: 0.0944 - accuracy: 0.9500 - val_loss: 0.0274 - val_accuracy: 1.0000\n",
      "Epoch 375/500\n",
      "120/120 [==============================] - 0s 366us/sample - loss: 0.0945 - accuracy: 0.9500 - val_loss: 0.0261 - val_accuracy: 1.0000\n",
      "Epoch 376/500\n",
      "120/120 [==============================] - 0s 366us/sample - loss: 0.0955 - accuracy: 0.9417 - val_loss: 0.0245 - val_accuracy: 1.0000\n",
      "Epoch 377/500\n",
      "120/120 [==============================] - 0s 399us/sample - loss: 0.0970 - accuracy: 0.9500 - val_loss: 0.0248 - val_accuracy: 1.0000\n",
      "Epoch 378/500\n",
      "120/120 [==============================] - 0s 454us/sample - loss: 0.1035 - accuracy: 0.9417 - val_loss: 0.0245 - val_accuracy: 1.0000\n",
      "Epoch 379/500\n",
      "120/120 [==============================] - 0s 497us/sample - loss: 0.0945 - accuracy: 0.9417 - val_loss: 0.0245 - val_accuracy: 1.0000\n",
      "Epoch 380/500\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.1424 - accuracy: 0.85 - 0s 572us/sample - loss: 0.0971 - accuracy: 0.9417 - val_loss: 0.0246 - val_accuracy: 1.0000\n",
      "Epoch 381/500\n",
      "120/120 [==============================] - 0s 457us/sample - loss: 0.0931 - accuracy: 0.9500 - val_loss: 0.0254 - val_accuracy: 1.0000\n",
      "Epoch 382/500\n",
      "120/120 [==============================] - 0s 557us/sample - loss: 0.0980 - accuracy: 0.9417 - val_loss: 0.0245 - val_accuracy: 1.0000\n",
      "Epoch 383/500\n",
      "120/120 [==============================] - 0s 407us/sample - loss: 0.0961 - accuracy: 0.9417 - val_loss: 0.0277 - val_accuracy: 1.0000\n",
      "Epoch 384/500\n",
      "120/120 [==============================] - 0s 424us/sample - loss: 0.0942 - accuracy: 0.9500 - val_loss: 0.0252 - val_accuracy: 1.0000\n",
      "Epoch 385/500\n",
      "120/120 [==============================] - 0s 424us/sample - loss: 0.0971 - accuracy: 0.9417 - val_loss: 0.0244 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 386/500\n",
      "120/120 [==============================] - 0s 374us/sample - loss: 0.0910 - accuracy: 0.9417 - val_loss: 0.0292 - val_accuracy: 1.0000\n",
      "Epoch 387/500\n",
      "120/120 [==============================] - 0s 632us/sample - loss: 0.0948 - accuracy: 0.9583 - val_loss: 0.0245 - val_accuracy: 1.0000\n",
      "Epoch 388/500\n",
      "120/120 [==============================] - 0s 524us/sample - loss: 0.0914 - accuracy: 0.9417 - val_loss: 0.0245 - val_accuracy: 1.0000\n",
      "Epoch 389/500\n",
      "120/120 [==============================] - 0s 557us/sample - loss: 0.0972 - accuracy: 0.9417 - val_loss: 0.0246 - val_accuracy: 1.0000\n",
      "Epoch 390/500\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.0027 - accuracy: 1.00 - 0s 387us/sample - loss: 0.0939 - accuracy: 0.9417 - val_loss: 0.0246 - val_accuracy: 1.0000\n",
      "Epoch 391/500\n",
      "120/120 [==============================] - 0s 507us/sample - loss: 0.0918 - accuracy: 0.9500 - val_loss: 0.0258 - val_accuracy: 1.0000\n",
      "Epoch 392/500\n",
      "120/120 [==============================] - 0s 418us/sample - loss: 0.0944 - accuracy: 0.9417 - val_loss: 0.0254 - val_accuracy: 1.0000\n",
      "Epoch 393/500\n",
      "120/120 [==============================] - 0s 487us/sample - loss: 0.0931 - accuracy: 0.9417 - val_loss: 0.0244 - val_accuracy: 1.0000\n",
      "Epoch 394/500\n",
      "120/120 [==============================] - 0s 395us/sample - loss: 0.0983 - accuracy: 0.9417 - val_loss: 0.0244 - val_accuracy: 1.0000\n",
      "Epoch 395/500\n",
      "120/120 [==============================] - 0s 407us/sample - loss: 0.0963 - accuracy: 0.9417 - val_loss: 0.0244 - val_accuracy: 1.0000\n",
      "Epoch 396/500\n",
      "120/120 [==============================] - 0s 482us/sample - loss: 0.0910 - accuracy: 0.9417 - val_loss: 0.0275 - val_accuracy: 1.0000\n",
      "Epoch 397/500\n",
      "120/120 [==============================] - 0s 382us/sample - loss: 0.0956 - accuracy: 0.9417 - val_loss: 0.0247 - val_accuracy: 1.0000\n",
      "Epoch 398/500\n",
      "120/120 [==============================] - 0s 382us/sample - loss: 0.0942 - accuracy: 0.9417 - val_loss: 0.0247 - val_accuracy: 1.0000\n",
      "Epoch 399/500\n",
      "120/120 [==============================] - 0s 382us/sample - loss: 0.0941 - accuracy: 0.9500 - val_loss: 0.0245 - val_accuracy: 1.0000\n",
      "Epoch 400/500\n",
      "120/120 [==============================] - 0s 374us/sample - loss: 0.0905 - accuracy: 0.9500 - val_loss: 0.0263 - val_accuracy: 1.0000\n",
      "Epoch 401/500\n",
      "120/120 [==============================] - 0s 432us/sample - loss: 0.0970 - accuracy: 0.9500 - val_loss: 0.0244 - val_accuracy: 1.0000\n",
      "Epoch 402/500\n",
      "120/120 [==============================] - 0s 463us/sample - loss: 0.0989 - accuracy: 0.9417 - val_loss: 0.0246 - val_accuracy: 1.0000\n",
      "Epoch 403/500\n",
      "120/120 [==============================] - 0s 444us/sample - loss: 0.0943 - accuracy: 0.9417 - val_loss: 0.0245 - val_accuracy: 1.0000\n",
      "Epoch 404/500\n",
      "120/120 [==============================] - 0s 382us/sample - loss: 0.0948 - accuracy: 0.9500 - val_loss: 0.0244 - val_accuracy: 1.0000\n",
      "Epoch 405/500\n",
      "120/120 [==============================] - 0s 540us/sample - loss: 0.0960 - accuracy: 0.9417 - val_loss: 0.0244 - val_accuracy: 1.0000\n",
      "Epoch 406/500\n",
      "120/120 [==============================] - 0s 478us/sample - loss: 0.0918 - accuracy: 0.9500 - val_loss: 0.0265 - val_accuracy: 1.0000\n",
      "Epoch 407/500\n",
      "120/120 [==============================] - 0s 424us/sample - loss: 0.0981 - accuracy: 0.9583 - val_loss: 0.0245 - val_accuracy: 1.0000\n",
      "Epoch 408/500\n",
      "120/120 [==============================] - 0s 410us/sample - loss: 0.0943 - accuracy: 0.9500 - val_loss: 0.0248 - val_accuracy: 1.0000\n",
      "Epoch 409/500\n",
      "120/120 [==============================] - 0s 366us/sample - loss: 0.0927 - accuracy: 0.9583 - val_loss: 0.0248 - val_accuracy: 1.0000\n",
      "Epoch 410/500\n",
      "120/120 [==============================] - 0s 416us/sample - loss: 0.0977 - accuracy: 0.9417 - val_loss: 0.0285 - val_accuracy: 1.0000\n",
      "Epoch 411/500\n",
      "120/120 [==============================] - 0s 457us/sample - loss: 0.0939 - accuracy: 0.9583 - val_loss: 0.0255 - val_accuracy: 1.0000\n",
      "Epoch 412/500\n",
      "120/120 [==============================] - 0s 382us/sample - loss: 0.0973 - accuracy: 0.9417 - val_loss: 0.0250 - val_accuracy: 1.0000\n",
      "Epoch 413/500\n",
      "120/120 [==============================] - 0s 399us/sample - loss: 0.0965 - accuracy: 0.9417 - val_loss: 0.0261 - val_accuracy: 1.0000\n",
      "Epoch 414/500\n",
      "120/120 [==============================] - 0s 391us/sample - loss: 0.0937 - accuracy: 0.9417 - val_loss: 0.0246 - val_accuracy: 1.0000\n",
      "Epoch 415/500\n",
      "120/120 [==============================] - 0s 490us/sample - loss: 0.0871 - accuracy: 0.9500 - val_loss: 0.0303 - val_accuracy: 1.0000\n",
      "Epoch 416/500\n",
      "120/120 [==============================] - 0s 382us/sample - loss: 0.0980 - accuracy: 0.9500 - val_loss: 0.0244 - val_accuracy: 1.0000\n",
      "Epoch 417/500\n",
      "120/120 [==============================] - 0s 657us/sample - loss: 0.0935 - accuracy: 0.9417 - val_loss: 0.0250 - val_accuracy: 1.0000\n",
      "Epoch 418/500\n",
      "120/120 [==============================] - 0s 461us/sample - loss: 0.0911 - accuracy: 0.9500 - val_loss: 0.0248 - val_accuracy: 1.0000\n",
      "Epoch 419/500\n",
      "120/120 [==============================] - 0s 439us/sample - loss: 0.0942 - accuracy: 0.9500 - val_loss: 0.0274 - val_accuracy: 1.0000\n",
      "Epoch 420/500\n",
      "120/120 [==============================] - 0s 440us/sample - loss: 0.0956 - accuracy: 0.9417 - val_loss: 0.0244 - val_accuracy: 1.0000\n",
      "Epoch 421/500\n",
      "120/120 [==============================] - 0s 441us/sample - loss: 0.0916 - accuracy: 0.9500 - val_loss: 0.0269 - val_accuracy: 1.0000\n",
      "Epoch 422/500\n",
      "120/120 [==============================] - 0s 424us/sample - loss: 0.0995 - accuracy: 0.9417 - val_loss: 0.0244 - val_accuracy: 1.0000\n",
      "Epoch 423/500\n",
      "120/120 [==============================] - 0s 427us/sample - loss: 0.0952 - accuracy: 0.9417 - val_loss: 0.0244 - val_accuracy: 1.0000\n",
      "Epoch 424/500\n",
      "120/120 [==============================] - 0s 414us/sample - loss: 0.0972 - accuracy: 0.9500 - val_loss: 0.0244 - val_accuracy: 1.0000\n",
      "Epoch 425/500\n",
      "120/120 [==============================] - 0s 460us/sample - loss: 0.0968 - accuracy: 0.9500 - val_loss: 0.0244 - val_accuracy: 1.0000\n",
      "Epoch 426/500\n",
      "120/120 [==============================] - 0s 391us/sample - loss: 0.0965 - accuracy: 0.9417 - val_loss: 0.0245 - val_accuracy: 1.0000\n",
      "Epoch 427/500\n",
      "120/120 [==============================] - 0s 426us/sample - loss: 0.0963 - accuracy: 0.9417 - val_loss: 0.0253 - val_accuracy: 1.0000\n",
      "Epoch 428/500\n",
      "120/120 [==============================] - 0s 440us/sample - loss: 0.0959 - accuracy: 0.9417 - val_loss: 0.0244 - val_accuracy: 1.0000\n",
      "Epoch 429/500\n",
      "120/120 [==============================] - 0s 391us/sample - loss: 0.0925 - accuracy: 0.9417 - val_loss: 0.0254 - val_accuracy: 1.0000\n",
      "Epoch 430/500\n",
      "120/120 [==============================] - 0s 399us/sample - loss: 0.0864 - accuracy: 0.9583 - val_loss: 0.0290 - val_accuracy: 1.0000\n",
      "Epoch 431/500\n",
      "120/120 [==============================] - 0s 433us/sample - loss: 0.0960 - accuracy: 0.9500 - val_loss: 0.0268 - val_accuracy: 1.0000\n",
      "Epoch 432/500\n",
      "120/120 [==============================] - 0s 449us/sample - loss: 0.0945 - accuracy: 0.9500 - val_loss: 0.0281 - val_accuracy: 1.0000\n",
      "Epoch 433/500\n",
      "120/120 [==============================] - 0s 416us/sample - loss: 0.0932 - accuracy: 0.9417 - val_loss: 0.0255 - val_accuracy: 1.0000\n",
      "Epoch 434/500\n",
      "120/120 [==============================] - 0s 366us/sample - loss: 0.0941 - accuracy: 0.9417 - val_loss: 0.0243 - val_accuracy: 1.0000\n",
      "Epoch 435/500\n",
      "120/120 [==============================] - 0s 399us/sample - loss: 0.0921 - accuracy: 0.9417 - val_loss: 0.0244 - val_accuracy: 1.0000\n",
      "Epoch 436/500\n",
      "120/120 [==============================] - 0s 465us/sample - loss: 0.0956 - accuracy: 0.9500 - val_loss: 0.0280 - val_accuracy: 1.0000\n",
      "Epoch 437/500\n",
      "120/120 [==============================] - 0s 374us/sample - loss: 0.0901 - accuracy: 0.9583 - val_loss: 0.0252 - val_accuracy: 1.0000\n",
      "Epoch 438/500\n",
      "120/120 [==============================] - 0s 396us/sample - loss: 0.0922 - accuracy: 0.9500 - val_loss: 0.0252 - val_accuracy: 1.0000\n",
      "Epoch 439/500\n",
      "120/120 [==============================] - 0s 399us/sample - loss: 0.1005 - accuracy: 0.9667 - val_loss: 0.0278 - val_accuracy: 1.0000\n",
      "Epoch 440/500\n",
      "120/120 [==============================] - 0s 416us/sample - loss: 0.0855 - accuracy: 0.9500 - val_loss: 0.0269 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 441/500\n",
      "120/120 [==============================] - 0s 382us/sample - loss: 0.0965 - accuracy: 0.9417 - val_loss: 0.0283 - val_accuracy: 1.0000\n",
      "Epoch 442/500\n",
      "120/120 [==============================] - 0s 413us/sample - loss: 0.0907 - accuracy: 0.9667 - val_loss: 0.0263 - val_accuracy: 1.0000\n",
      "Epoch 443/500\n",
      "120/120 [==============================] - 0s 457us/sample - loss: 0.0971 - accuracy: 0.9500 - val_loss: 0.0244 - val_accuracy: 1.0000\n",
      "Epoch 444/500\n",
      "120/120 [==============================] - 0s 482us/sample - loss: 0.0968 - accuracy: 0.9417 - val_loss: 0.0243 - val_accuracy: 1.0000\n",
      "Epoch 445/500\n",
      "120/120 [==============================] - 0s 366us/sample - loss: 0.0991 - accuracy: 0.9417 - val_loss: 0.0243 - val_accuracy: 1.0000\n",
      "Epoch 446/500\n",
      "120/120 [==============================] - 0s 482us/sample - loss: 0.0916 - accuracy: 0.9417 - val_loss: 0.0243 - val_accuracy: 1.0000\n",
      "Epoch 447/500\n",
      "120/120 [==============================] - 0s 448us/sample - loss: 0.0962 - accuracy: 0.9500 - val_loss: 0.0251 - val_accuracy: 1.0000\n",
      "Epoch 448/500\n",
      "120/120 [==============================] - 0s 443us/sample - loss: 0.0939 - accuracy: 0.9417 - val_loss: 0.0243 - val_accuracy: 1.0000\n",
      "Epoch 449/500\n",
      "120/120 [==============================] - 0s 349us/sample - loss: 0.0952 - accuracy: 0.9417 - val_loss: 0.0249 - val_accuracy: 1.0000\n",
      "Epoch 450/500\n",
      "120/120 [==============================] - 0s 349us/sample - loss: 0.0950 - accuracy: 0.9500 - val_loss: 0.0248 - val_accuracy: 1.0000\n",
      "Epoch 451/500\n",
      "120/120 [==============================] - 0s 399us/sample - loss: 0.0945 - accuracy: 0.9500 - val_loss: 0.0248 - val_accuracy: 1.0000\n",
      "Epoch 452/500\n",
      "120/120 [==============================] - 0s 351us/sample - loss: 0.0916 - accuracy: 0.9500 - val_loss: 0.0252 - val_accuracy: 1.0000\n",
      "Epoch 453/500\n",
      "120/120 [==============================] - 0s 374us/sample - loss: 0.0976 - accuracy: 0.9417 - val_loss: 0.0251 - val_accuracy: 1.0000\n",
      "Epoch 454/500\n",
      "120/120 [==============================] - 0s 440us/sample - loss: 0.0946 - accuracy: 0.9417 - val_loss: 0.0273 - val_accuracy: 1.0000\n",
      "Epoch 455/500\n",
      "120/120 [==============================] - 0s 365us/sample - loss: 0.0912 - accuracy: 0.9500 - val_loss: 0.0249 - val_accuracy: 1.0000\n",
      "Epoch 456/500\n",
      "120/120 [==============================] - 0s 425us/sample - loss: 0.0854 - accuracy: 0.9583 - val_loss: 0.0304 - val_accuracy: 1.0000\n",
      "Epoch 457/500\n",
      "120/120 [==============================] - 0s 369us/sample - loss: 0.0960 - accuracy: 0.9583 - val_loss: 0.0246 - val_accuracy: 1.0000\n",
      "Epoch 458/500\n",
      "120/120 [==============================] - 0s 449us/sample - loss: 0.0957 - accuracy: 0.9417 - val_loss: 0.0264 - val_accuracy: 1.0000\n",
      "Epoch 459/500\n",
      "120/120 [==============================] - 0s 461us/sample - loss: 0.0875 - accuracy: 0.9500 - val_loss: 0.0252 - val_accuracy: 1.0000\n",
      "Epoch 460/500\n",
      "120/120 [==============================] - 0s 499us/sample - loss: 0.0997 - accuracy: 0.9417 - val_loss: 0.0242 - val_accuracy: 1.0000\n",
      "Epoch 461/500\n",
      "120/120 [==============================] - 0s 391us/sample - loss: 0.0919 - accuracy: 0.9500 - val_loss: 0.0247 - val_accuracy: 1.0000\n",
      "Epoch 462/500\n",
      "120/120 [==============================] - 0s 366us/sample - loss: 0.0938 - accuracy: 0.9417 - val_loss: 0.0243 - val_accuracy: 1.0000\n",
      "Epoch 463/500\n",
      "120/120 [==============================] - 0s 407us/sample - loss: 0.0935 - accuracy: 0.9500 - val_loss: 0.0245 - val_accuracy: 1.0000\n",
      "Epoch 464/500\n",
      "120/120 [==============================] - 0s 474us/sample - loss: 0.0995 - accuracy: 0.9417 - val_loss: 0.0243 - val_accuracy: 1.0000\n",
      "Epoch 465/500\n",
      "120/120 [==============================] - 0s 366us/sample - loss: 0.0916 - accuracy: 0.9417 - val_loss: 0.0246 - val_accuracy: 1.0000\n",
      "Epoch 466/500\n",
      "120/120 [==============================] - 0s 366us/sample - loss: 0.0978 - accuracy: 0.9500 - val_loss: 0.0250 - val_accuracy: 1.0000\n",
      "Epoch 467/500\n",
      "120/120 [==============================] - 0s 382us/sample - loss: 0.0959 - accuracy: 0.9417 - val_loss: 0.0244 - val_accuracy: 1.0000\n",
      "Epoch 468/500\n",
      "120/120 [==============================] - 0s 366us/sample - loss: 0.0944 - accuracy: 0.9417 - val_loss: 0.0242 - val_accuracy: 1.0000\n",
      "Epoch 469/500\n",
      "120/120 [==============================] - 0s 383us/sample - loss: 0.0916 - accuracy: 0.9500 - val_loss: 0.0247 - val_accuracy: 1.0000\n",
      "Epoch 470/500\n",
      "120/120 [==============================] - 0s 374us/sample - loss: 0.0929 - accuracy: 0.9500 - val_loss: 0.0324 - val_accuracy: 1.0000\n",
      "Epoch 471/500\n",
      "120/120 [==============================] - 0s 391us/sample - loss: 0.0977 - accuracy: 0.9583 - val_loss: 0.0243 - val_accuracy: 1.0000\n",
      "Epoch 472/500\n",
      "120/120 [==============================] - 0s 374us/sample - loss: 0.0951 - accuracy: 0.9583 - val_loss: 0.0242 - val_accuracy: 1.0000\n",
      "Epoch 473/500\n",
      "120/120 [==============================] - 0s 428us/sample - loss: 0.0960 - accuracy: 0.9500 - val_loss: 0.0244 - val_accuracy: 1.0000\n",
      "Epoch 474/500\n",
      "120/120 [==============================] - 0s 366us/sample - loss: 0.0983 - accuracy: 0.9500 - val_loss: 0.0262 - val_accuracy: 1.0000\n",
      "Epoch 475/500\n",
      "120/120 [==============================] - 0s 374us/sample - loss: 0.0960 - accuracy: 0.9417 - val_loss: 0.0247 - val_accuracy: 1.0000\n",
      "Epoch 476/500\n",
      "120/120 [==============================] - 0s 388us/sample - loss: 0.0980 - accuracy: 0.9417 - val_loss: 0.0249 - val_accuracy: 1.0000\n",
      "Epoch 477/500\n",
      "120/120 [==============================] - 0s 440us/sample - loss: 0.0875 - accuracy: 0.9583 - val_loss: 0.0269 - val_accuracy: 1.0000\n",
      "Epoch 478/500\n",
      "120/120 [==============================] - 0s 383us/sample - loss: 0.0982 - accuracy: 0.9417 - val_loss: 0.0282 - val_accuracy: 1.0000\n",
      "Epoch 479/500\n",
      "120/120 [==============================] - 0s 373us/sample - loss: 0.0956 - accuracy: 0.9500 - val_loss: 0.0241 - val_accuracy: 1.0000\n",
      "Epoch 480/500\n",
      "120/120 [==============================] - 0s 357us/sample - loss: 0.0986 - accuracy: 0.9417 - val_loss: 0.0242 - val_accuracy: 1.0000\n",
      "Epoch 481/500\n",
      "120/120 [==============================] - 0s 391us/sample - loss: 0.0927 - accuracy: 0.9500 - val_loss: 0.0242 - val_accuracy: 1.0000\n",
      "Epoch 482/500\n",
      "120/120 [==============================] - 0s 420us/sample - loss: 0.0914 - accuracy: 0.9500 - val_loss: 0.0341 - val_accuracy: 0.9667\n",
      "Epoch 483/500\n",
      "120/120 [==============================] - 0s 366us/sample - loss: 0.0979 - accuracy: 0.9583 - val_loss: 0.0249 - val_accuracy: 1.0000\n",
      "Epoch 484/500\n",
      "120/120 [==============================] - 0s 357us/sample - loss: 0.0910 - accuracy: 0.9417 - val_loss: 0.0252 - val_accuracy: 1.0000\n",
      "Epoch 485/500\n",
      "120/120 [==============================] - 0s 424us/sample - loss: 0.0979 - accuracy: 0.9417 - val_loss: 0.0243 - val_accuracy: 1.0000\n",
      "Epoch 486/500\n",
      "120/120 [==============================] - 0s 368us/sample - loss: 0.0941 - accuracy: 0.9500 - val_loss: 0.0256 - val_accuracy: 1.0000\n",
      "Epoch 487/500\n",
      "120/120 [==============================] - 0s 382us/sample - loss: 0.0975 - accuracy: 0.9417 - val_loss: 0.0241 - val_accuracy: 1.0000\n",
      "Epoch 488/500\n",
      "120/120 [==============================] - 0s 382us/sample - loss: 0.0928 - accuracy: 0.9417 - val_loss: 0.0252 - val_accuracy: 1.0000\n",
      "Epoch 489/500\n",
      "120/120 [==============================] - 0s 440us/sample - loss: 0.0987 - accuracy: 0.9417 - val_loss: 0.0244 - val_accuracy: 1.0000\n",
      "Epoch 490/500\n",
      "120/120 [==============================] - 0s 357us/sample - loss: 0.0952 - accuracy: 0.9500 - val_loss: 0.0245 - val_accuracy: 1.0000\n",
      "Epoch 491/500\n",
      "120/120 [==============================] - 0s 358us/sample - loss: 0.0964 - accuracy: 0.9500 - val_loss: 0.0248 - val_accuracy: 1.0000\n",
      "Epoch 492/500\n",
      "120/120 [==============================] - 0s 432us/sample - loss: 0.0923 - accuracy: 0.9417 - val_loss: 0.0253 - val_accuracy: 1.0000\n",
      "Epoch 493/500\n",
      "120/120 [==============================] - 0s 374us/sample - loss: 0.0985 - accuracy: 0.9417 - val_loss: 0.0244 - val_accuracy: 1.0000\n",
      "Epoch 494/500\n",
      "120/120 [==============================] - 0s 366us/sample - loss: 0.0897 - accuracy: 0.9500 - val_loss: 0.0257 - val_accuracy: 1.0000\n",
      "Epoch 495/500\n",
      "120/120 [==============================] - 0s 375us/sample - loss: 0.0952 - accuracy: 0.9500 - val_loss: 0.0243 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 496/500\n",
      "120/120 [==============================] - 0s 368us/sample - loss: 0.0933 - accuracy: 0.9500 - val_loss: 0.0249 - val_accuracy: 1.0000\n",
      "Epoch 497/500\n",
      "120/120 [==============================] - 0s 381us/sample - loss: 0.0937 - accuracy: 0.9500 - val_loss: 0.0245 - val_accuracy: 1.0000\n",
      "Epoch 498/500\n",
      "120/120 [==============================] - 0s 458us/sample - loss: 0.0953 - accuracy: 0.9417 - val_loss: 0.0257 - val_accuracy: 1.0000\n",
      "Epoch 499/500\n",
      "120/120 [==============================] - 0s 515us/sample - loss: 0.0949 - accuracy: 0.9417 - val_loss: 0.0248 - val_accuracy: 1.0000\n",
      "Epoch 500/500\n",
      "120/120 [==============================] - 0s 424us/sample - loss: 0.0941 - accuracy: 0.9417 - val_loss: 0.0243 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "History_iris = Model_iris.fit(x_train, y_train,\n",
    "                              epochs = 500,\n",
    "                              batch_size = 7,\n",
    "                              validation_data = (x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.09914362992858514,\n",
       "  0.09731556298599268,\n",
       "  0.10043200870665411,\n",
       "  0.09660003252599078,\n",
       "  0.09658946446531141,\n",
       "  0.09612290872748114,\n",
       "  0.0988323642978988,\n",
       "  0.09983078250758505,\n",
       "  0.09641803421157723,\n",
       "  0.09944297063899891,\n",
       "  0.09978091916346633,\n",
       "  0.0981568698422052,\n",
       "  0.10020909388161575,\n",
       "  0.09942009375275423,\n",
       "  0.09450038489982641,\n",
       "  0.0979001295777683,\n",
       "  0.09860366903400669,\n",
       "  0.09556252333042418,\n",
       "  0.09746912333954848,\n",
       "  0.09673321958147578,\n",
       "  0.09400948882151473,\n",
       "  0.09806867753635894,\n",
       "  0.09611798267254319,\n",
       "  0.09885309150862061,\n",
       "  0.09902655178991608,\n",
       "  0.09831455205076053,\n",
       "  0.09455489028987359,\n",
       "  0.09292453368643691,\n",
       "  0.09968598039704375,\n",
       "  0.09845067514882734,\n",
       "  0.09565599087424441,\n",
       "  0.09720784510136582,\n",
       "  0.10053720216067935,\n",
       "  0.10074344703226264,\n",
       "  0.09839611692041217,\n",
       "  0.0969687561931399,\n",
       "  0.10053906224482413,\n",
       "  0.1002011662494624,\n",
       "  0.09482572043156476,\n",
       "  0.09635437334654853,\n",
       "  0.09914667113819936,\n",
       "  0.09563509423072294,\n",
       "  0.09709626335922318,\n",
       "  0.09779701317214252,\n",
       "  0.10290657410441781,\n",
       "  0.09591732138166359,\n",
       "  0.09507674709335939,\n",
       "  0.10033200899900598,\n",
       "  0.10315317235460195,\n",
       "  0.09556165440565868,\n",
       "  0.09648678099138124,\n",
       "  0.09551462763297422,\n",
       "  0.10019611442273041,\n",
       "  0.10191793170912812,\n",
       "  0.10064498122277049,\n",
       "  0.09510499523191053,\n",
       "  0.10049509139498695,\n",
       "  0.09112802528640411,\n",
       "  0.09767439313970196,\n",
       "  0.09837993438947402,\n",
       "  0.09349461222009267,\n",
       "  0.09901841872451769,\n",
       "  0.09438261378187841,\n",
       "  0.0970260237739088,\n",
       "  0.0993122126868305,\n",
       "  0.09893868737428117,\n",
       "  0.09616694249076924,\n",
       "  0.09767832627888613,\n",
       "  0.0977464395070759,\n",
       "  0.09283053284452762,\n",
       "  0.09896307450632852,\n",
       "  0.09630008904108157,\n",
       "  0.09875620017992333,\n",
       "  0.10006516181116847,\n",
       "  0.09586784428101965,\n",
       "  0.09589227255977069,\n",
       "  0.09874172783652284,\n",
       "  0.09532126040139702,\n",
       "  0.10250608810422364,\n",
       "  0.10005446449164689,\n",
       "  0.09780514906985142,\n",
       "  0.09317360329016537,\n",
       "  0.09402317667017997,\n",
       "  0.10087983184251546,\n",
       "  0.0919393941439921,\n",
       "  0.09795307490373185,\n",
       "  0.09845048167238322,\n",
       "  0.09357438942727943,\n",
       "  0.0987033100410675,\n",
       "  0.09932276023270485,\n",
       "  0.09666438130419314,\n",
       "  0.09836245846278567,\n",
       "  0.09898584548888417,\n",
       "  0.09854748335880383,\n",
       "  0.09589749013296872,\n",
       "  0.09939674414781621,\n",
       "  0.09741619372216519,\n",
       "  0.09619440818981578,\n",
       "  0.09617049702166393,\n",
       "  0.09404331253802714,\n",
       "  0.09494245289388345,\n",
       "  0.09903603383548519,\n",
       "  0.09594141045139017,\n",
       "  0.09647060489279889,\n",
       "  0.0981605732625629,\n",
       "  0.0958513028609256,\n",
       "  0.09561052730610224,\n",
       "  0.09696116807463113,\n",
       "  0.09323898901218779,\n",
       "  0.09620817564830304,\n",
       "  0.09443082859894882,\n",
       "  0.09703543342161539,\n",
       "  0.0918200622535854,\n",
       "  0.09666777991418106,\n",
       "  0.09666265666019172,\n",
       "  0.09578411539259832,\n",
       "  0.09769366915182522,\n",
       "  0.09574027912146145,\n",
       "  0.0971699653744281,\n",
       "  0.09804550859019703,\n",
       "  0.0980314724916146,\n",
       "  0.09812506916292704,\n",
       "  0.09462128096371696,\n",
       "  0.09860744784811762,\n",
       "  0.0961645335985357,\n",
       "  0.09352654752826008,\n",
       "  0.0958888405911921,\n",
       "  0.08958464223736276,\n",
       "  0.0971098801024103,\n",
       "  0.09613554863573578,\n",
       "  0.09761351474987805,\n",
       "  0.09100476943616134,\n",
       "  0.09647247657970487,\n",
       "  0.09711389729442696,\n",
       "  0.09633767503134247,\n",
       "  0.09851895550673362,\n",
       "  0.09874883394028681,\n",
       "  0.09092498126788087,\n",
       "  0.09822976353704387,\n",
       "  0.09413503834754616,\n",
       "  0.09806542158465466,\n",
       "  0.09358992223260808,\n",
       "  0.09783464110126563,\n",
       "  0.09723066513737043,\n",
       "  0.09178360685812853,\n",
       "  0.09479503592301626,\n",
       "  0.09519101115001831,\n",
       "  0.094705888342772,\n",
       "  0.09585115186249217,\n",
       "  0.09986642846081016,\n",
       "  0.09457602986464432,\n",
       "  0.09718052898388123,\n",
       "  0.09342963905752792,\n",
       "  0.08809683927713649,\n",
       "  0.09725562438707887,\n",
       "  0.09804808647591017,\n",
       "  0.09979120466935759,\n",
       "  0.09573892461000165,\n",
       "  0.09160147788206814,\n",
       "  0.09795354751771962,\n",
       "  0.09769632462218093,\n",
       "  0.09284191744178921,\n",
       "  0.09975739993286273,\n",
       "  0.09492714481602889,\n",
       "  0.09913436793419805,\n",
       "  0.0934261770239876,\n",
       "  0.08979897130435953,\n",
       "  0.09537347373580664,\n",
       "  0.09580963933785824,\n",
       "  0.09621284778841073,\n",
       "  0.09605136341415346,\n",
       "  0.09651281338287086,\n",
       "  0.0958331608383105,\n",
       "  0.10224790681095328,\n",
       "  0.09704798846601079,\n",
       "  0.09719473837249097,\n",
       "  0.09657159175306636,\n",
       "  0.09629481884233732,\n",
       "  0.09432549682163274,\n",
       "  0.09167931928401458,\n",
       "  0.09575227526233902,\n",
       "  0.09457569931573744,\n",
       "  0.10038266151871843,\n",
       "  0.09280583251367412,\n",
       "  0.09375588081893511,\n",
       "  0.09909989720811913,\n",
       "  0.09771806176857277,\n",
       "  0.09651595998099462,\n",
       "  0.09547386627576392,\n",
       "  0.092964365464771,\n",
       "  0.0945787735991568,\n",
       "  0.09798258775828496,\n",
       "  0.09485427381091541,\n",
       "  0.09837205529523392,\n",
       "  0.09327028455736582,\n",
       "  0.09660260580712929,\n",
       "  0.09604836700067002,\n",
       "  0.09789690211279473,\n",
       "  0.09643671634435881,\n",
       "  0.0951990030434293,\n",
       "  0.09330355374841019,\n",
       "  0.08847837312108216,\n",
       "  0.09588749959450524,\n",
       "  0.09432994427912339,\n",
       "  0.10081278054955571,\n",
       "  0.09680286750905603,\n",
       "  0.09817423880303977,\n",
       "  0.09175256190064829,\n",
       "  0.09456829772277615,\n",
       "  0.09622220655510319,\n",
       "  0.09882813379736036,\n",
       "  0.09333946075442251,\n",
       "  0.08910894478497464,\n",
       "  0.09745075337996241,\n",
       "  0.0970677406701725,\n",
       "  0.09205674051966829,\n",
       "  0.09781388986557431,\n",
       "  0.09339240698706514,\n",
       "  0.09428983049486608,\n",
       "  0.09449178486222157,\n",
       "  0.09748825051744157,\n",
       "  0.09774755092988699,\n",
       "  0.09613309608636579,\n",
       "  0.0925146338888832,\n",
       "  0.09192278478197598,\n",
       "  0.09593173823401836,\n",
       "  0.09446830648812465,\n",
       "  0.09541161410234812,\n",
       "  0.09426569701022913,\n",
       "  0.09160735739145215,\n",
       "  0.10067293712151391,\n",
       "  0.09705136739309334,\n",
       "  0.09290645822176202,\n",
       "  0.09674650523860086,\n",
       "  0.0943241425496429,\n",
       "  0.09716054589650108,\n",
       "  0.09359699895333809,\n",
       "  0.09593380889103477,\n",
       "  0.09432734151963208,\n",
       "  0.097524023204096,\n",
       "  0.09764553247853959,\n",
       "  0.0980873827727919,\n",
       "  0.09374559024581686,\n",
       "  0.09366770467434739,\n",
       "  0.09499996550924455,\n",
       "  0.09718123003840447,\n",
       "  0.09512247180575742,\n",
       "  0.09516058682832712,\n",
       "  0.09519581627415998,\n",
       "  0.09641102779035766,\n",
       "  0.09528333600217517,\n",
       "  0.09620261682733447,\n",
       "  0.09563364850649046,\n",
       "  0.09966435441056849,\n",
       "  0.0927752030310027,\n",
       "  0.09549140702826359,\n",
       "  0.09456383696173513,\n",
       "  0.09387421196685561,\n",
       "  0.09152940923148284,\n",
       "  0.09457415644028515,\n",
       "  0.09437564921088323,\n",
       "  0.09823216021177358,\n",
       "  0.10183078818760502,\n",
       "  0.09265248056520553,\n",
       "  0.0920209994538709,\n",
       "  0.0951952231440714,\n",
       "  0.0963875886554888,\n",
       "  0.09692662848674824,\n",
       "  0.09347474935879063,\n",
       "  0.09633106299532983,\n",
       "  0.09611589876779665,\n",
       "  0.09293759544884021,\n",
       "  0.09814559941393479,\n",
       "  0.09357563878293149,\n",
       "  0.096967224831,\n",
       "  0.09715898995297417,\n",
       "  0.09803346567011127,\n",
       "  0.09489752559068923,\n",
       "  0.09491186839646465,\n",
       "  0.09577036679984303,\n",
       "  0.09584432904081837,\n",
       "  0.09508980823835979,\n",
       "  0.09478544330656101,\n",
       "  0.09493141914135776,\n",
       "  0.0945413050930559,\n",
       "  0.09731033884066467,\n",
       "  0.09822446428394566,\n",
       "  0.09335105488280533,\n",
       "  0.09213867953115672,\n",
       "  0.09902523657826047,\n",
       "  0.09129258010652848,\n",
       "  0.09136079009016006,\n",
       "  0.0973923800852693,\n",
       "  0.09571079957975902,\n",
       "  0.09885800941459214,\n",
       "  0.09541005246768085,\n",
       "  0.09284221451137759,\n",
       "  0.0951361359533621,\n",
       "  0.09530388125368745,\n",
       "  0.09436412657608646,\n",
       "  0.09298981943966282,\n",
       "  0.09903123047188274,\n",
       "  0.09302333423135375,\n",
       "  0.09807472089475293,\n",
       "  0.09608748414997119,\n",
       "  0.0884124105086471,\n",
       "  0.0998056690878002,\n",
       "  0.09399144041429584,\n",
       "  0.0962392676466455,\n",
       "  0.09615842364573078,\n",
       "  0.09617056639059834,\n",
       "  0.09243636254929394,\n",
       "  0.09450997272884934,\n",
       "  0.093876284641154,\n",
       "  0.09055862196734096,\n",
       "  0.09680639442049141,\n",
       "  0.09664529700494313,\n",
       "  0.09492592640690418,\n",
       "  0.09484898656082805,\n",
       "  0.09096214964174958,\n",
       "  0.09473956536239711,\n",
       "  0.091823086218695,\n",
       "  0.09710322033740036,\n",
       "  0.09661492282490751,\n",
       "  0.09360057079366962,\n",
       "  0.09569221776570581,\n",
       "  0.10361361643050865,\n",
       "  0.08883972620913407,\n",
       "  0.09454061667250548,\n",
       "  0.09379480846006345,\n",
       "  0.09397482834562348,\n",
       "  0.0975900317634417,\n",
       "  0.0971490869060896,\n",
       "  0.0917588168507791,\n",
       "  0.08650680083307331,\n",
       "  0.09975444868032354,\n",
       "  0.09778109819514308,\n",
       "  0.10068051044824339,\n",
       "  0.09474962918599582,\n",
       "  0.09470523844477914,\n",
       "  0.09833893169125077,\n",
       "  0.09726183400780428,\n",
       "  0.09362804942470575,\n",
       "  0.09232034790909438,\n",
       "  0.09747939369766148,\n",
       "  0.09420683441445969,\n",
       "  0.0949342381907627,\n",
       "  0.09498362499046682,\n",
       "  0.09528016069913671,\n",
       "  0.0845989153234882,\n",
       "  0.09820909234549617,\n",
       "  0.09749823460867144,\n",
       "  0.09367393233502905,\n",
       "  0.09425411184395974,\n",
       "  0.09736361300730323,\n",
       "  0.09184778765496351,\n",
       "  0.09674475993451778,\n",
       "  0.09504289591561702,\n",
       "  0.09731081707935421,\n",
       "  0.0976907574358241,\n",
       "  0.09496561582677714,\n",
       "  0.0952159834443061,\n",
       "  0.09876432015633631,\n",
       "  0.10085811372160833,\n",
       "  0.088651845951487,\n",
       "  0.09704381644388226,\n",
       "  0.09683562765902327,\n",
       "  0.1001278115902096,\n",
       "  0.09472404472617199,\n",
       "  0.0870238844382565,\n",
       "  0.0952054734952848,\n",
       "  0.09598063793862745,\n",
       "  0.09805598784486923,\n",
       "  0.09441715876649444,\n",
       "  0.09454161072183827,\n",
       "  0.09547023812046973,\n",
       "  0.09703812009344498,\n",
       "  0.10345889678570189,\n",
       "  0.09452717194455242,\n",
       "  0.09714738743011063,\n",
       "  0.09306134109368334,\n",
       "  0.09801091107947286,\n",
       "  0.09611017838809251,\n",
       "  0.09421767950079811,\n",
       "  0.09706750966361141,\n",
       "  0.09101232034445275,\n",
       "  0.09479024566123068,\n",
       "  0.09141184978264695,\n",
       "  0.09716291875611584,\n",
       "  0.09386877368813581,\n",
       "  0.09182976786969826,\n",
       "  0.0944210083160821,\n",
       "  0.09314913549460471,\n",
       "  0.09826742148499458,\n",
       "  0.09632166053634136,\n",
       "  0.09103701248726187,\n",
       "  0.0955841317268399,\n",
       "  0.0941684802096006,\n",
       "  0.09409803784024007,\n",
       "  0.09047013389229998,\n",
       "  0.09696615168310624,\n",
       "  0.09887421027912448,\n",
       "  0.09432398398057558,\n",
       "  0.09484938258389093,\n",
       "  0.09595407780143432,\n",
       "  0.09176684124873115,\n",
       "  0.09807476080798855,\n",
       "  0.09430665789400526,\n",
       "  0.09266509642159994,\n",
       "  0.09772021926667852,\n",
       "  0.09386381041901283,\n",
       "  0.09734700183325913,\n",
       "  0.09651640860916662,\n",
       "  0.0936723582256036,\n",
       "  0.08706384176924378,\n",
       "  0.09799257995885759,\n",
       "  0.09352969498819827,\n",
       "  0.09107647433490153,\n",
       "  0.09424675842407547,\n",
       "  0.0956215066408049,\n",
       "  0.09164999211704222,\n",
       "  0.09952643084883069,\n",
       "  0.0952216603336808,\n",
       "  0.0971751134488045,\n",
       "  0.0967546936085076,\n",
       "  0.09654122815321671,\n",
       "  0.09630688677813547,\n",
       "  0.09592519362771175,\n",
       "  0.0924889231576041,\n",
       "  0.08635363327290785,\n",
       "  0.0959574930651191,\n",
       "  0.09445630978395153,\n",
       "  0.09323899134857735,\n",
       "  0.09407833847589776,\n",
       "  0.09207681157560804,\n",
       "  0.09555911310199008,\n",
       "  0.09005342917709716,\n",
       "  0.0921555817164214,\n",
       "  0.10054470036023606,\n",
       "  0.08551798234839225,\n",
       "  0.09654286622147386,\n",
       "  0.0906761103484314,\n",
       "  0.09705831390986835,\n",
       "  0.09677731640016039,\n",
       "  0.09906043204658393,\n",
       "  0.09159963908338492,\n",
       "  0.09615148773009423,\n",
       "  0.09390751522247835,\n",
       "  0.095220059021752,\n",
       "  0.09501531834927543,\n",
       "  0.09453077350283365,\n",
       "  0.09156077445513802,\n",
       "  0.09763352389491047,\n",
       "  0.09458339118929189,\n",
       "  0.09124861888976739,\n",
       "  0.08538623128633654,\n",
       "  0.09600844570127265,\n",
       "  0.09566227424268921,\n",
       "  0.08746047029544192,\n",
       "  0.09965655871928902,\n",
       "  0.09193548852005809,\n",
       "  0.09382827533790229,\n",
       "  0.09346518628735793,\n",
       "  0.09947229853275037,\n",
       "  0.09155903129033428,\n",
       "  0.09777769491968986,\n",
       "  0.09587013361837458,\n",
       "  0.09436858439633701,\n",
       "  0.09158303335656796,\n",
       "  0.09290069473984962,\n",
       "  0.09768695994765343,\n",
       "  0.09510725147556513,\n",
       "  0.09604503483545462,\n",
       "  0.09827690024709833,\n",
       "  0.09599635257084932,\n",
       "  0.09798028183480104,\n",
       "  0.08751888000321438,\n",
       "  0.09820438503811602,\n",
       "  0.09556880190454346,\n",
       "  0.09861192924630208,\n",
       "  0.09272211968900214,\n",
       "  0.09136068773223087,\n",
       "  0.09789209900166801,\n",
       "  0.09104143356926216,\n",
       "  0.09791517466583173,\n",
       "  0.09407245379208955,\n",
       "  0.09746037551763796,\n",
       "  0.092754514023909,\n",
       "  0.09866969021992797,\n",
       "  0.09522476842466858,\n",
       "  0.0964321132118736,\n",
       "  0.09234229615079433,\n",
       "  0.0984789366077166,\n",
       "  0.0896658953410224,\n",
       "  0.09524124909827757,\n",
       "  0.09332091587863639,\n",
       "  0.09369984607716712,\n",
       "  0.09531186358030179,\n",
       "  0.09492367310313246,\n",
       "  0.09412659238660126],\n",
       " 'accuracy': [0.95,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.95,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.9583333,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.9583333,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.9583333,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.9583333,\n",
       "  0.95,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.95,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.95,\n",
       "  0.95,\n",
       "  0.9583333,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.95,\n",
       "  0.9583333,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.9583333,\n",
       "  0.9583333,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.95,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.9583333,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.96666664,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.95,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.96666664,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.9583333,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.9583333,\n",
       "  0.9583333,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.9583333,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.96666664,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.95,\n",
       "  0.95,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.9583333,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.9583333,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.95,\n",
       "  0.9583333,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.95,\n",
       "  0.9583333,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.9583333,\n",
       "  0.95,\n",
       "  0.95,\n",
       "  0.95,\n",
       "  0.95,\n",
       "  0.9583333,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.9583333,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.9583333,\n",
       "  0.94166666,\n",
       "  0.9583333,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.9583333,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.9583333,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.9583333,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.9583333,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.9583333,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.95,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.95,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.9583333,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.95,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.9583333,\n",
       "  0.95,\n",
       "  0.9583333,\n",
       "  0.94166666,\n",
       "  0.9583333,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.9583333,\n",
       "  0.95,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.9583333,\n",
       "  0.95,\n",
       "  0.96666664,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.96666664,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.95,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.9583333,\n",
       "  0.9583333,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.95,\n",
       "  0.9583333,\n",
       "  0.9583333,\n",
       "  0.95,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.9583333,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.95,\n",
       "  0.9583333,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.95,\n",
       "  0.95,\n",
       "  0.95,\n",
       "  0.95,\n",
       "  0.94166666,\n",
       "  0.94166666,\n",
       "  0.94166666],\n",
       " 'val_loss': [0.031208990747109054,\n",
       "  0.03228576217467586,\n",
       "  0.02970261595522364,\n",
       "  0.033440898482998214,\n",
       "  0.02923192539407561,\n",
       "  0.029063636204227805,\n",
       "  0.029012914304621516,\n",
       "  0.029753695028678823,\n",
       "  0.028950514302899442,\n",
       "  0.02921902302186936,\n",
       "  0.028902469210637112,\n",
       "  0.030838957025359073,\n",
       "  0.03585559248458594,\n",
       "  0.028829638209814826,\n",
       "  0.033311409056962775,\n",
       "  0.029748840940495333,\n",
       "  0.02947452381098022,\n",
       "  0.029383408623592306,\n",
       "  0.0289581534685567,\n",
       "  0.029349419575495025,\n",
       "  0.0347472011577338,\n",
       "  0.02895876606926322,\n",
       "  0.030641500566465158,\n",
       "  0.028633714009386798,\n",
       "  0.029864098643884063,\n",
       "  0.028997193703738353,\n",
       "  0.02857322001364082,\n",
       "  0.029933170423222084,\n",
       "  0.028457790597652396,\n",
       "  0.03234056575068583,\n",
       "  0.028758824764130015,\n",
       "  0.029417329902450242,\n",
       "  0.028550806140992792,\n",
       "  0.028310163831338286,\n",
       "  0.028390272931816677,\n",
       "  0.02932889920193702,\n",
       "  0.028866119023102026,\n",
       "  0.032283703501646714,\n",
       "  0.028265619254671037,\n",
       "  0.032302587622931846,\n",
       "  0.028318752410511176,\n",
       "  0.02863749008004864,\n",
       "  0.029702362588917217,\n",
       "  0.028123262974744042,\n",
       "  0.028232983763640127,\n",
       "  0.029449172438277552,\n",
       "  0.03691421321903666,\n",
       "  0.03179667614555607,\n",
       "  0.028377737430855633,\n",
       "  0.027997857231336336,\n",
       "  0.028348589347054563,\n",
       "  0.02940676549527173,\n",
       "  0.028468485494765144,\n",
       "  0.03269939433860903,\n",
       "  0.028691614461907496,\n",
       "  0.029180247941985725,\n",
       "  0.02970984304168572,\n",
       "  0.03104997080905984,\n",
       "  0.027811972292450568,\n",
       "  0.02958950767448793,\n",
       "  0.029337697313167155,\n",
       "  0.02796771445622047,\n",
       "  0.029027694700441014,\n",
       "  0.027982957583541673,\n",
       "  0.03173457880038768,\n",
       "  0.027596461547849078,\n",
       "  0.030737102951388807,\n",
       "  0.02812958414045473,\n",
       "  0.027576869710658987,\n",
       "  0.030584386239449184,\n",
       "  0.02757470888706545,\n",
       "  0.027490490574079256,\n",
       "  0.0295660494438683,\n",
       "  0.02743822803798442,\n",
       "  0.027497187334423263,\n",
       "  0.027489821694325657,\n",
       "  0.027870220597833395,\n",
       "  0.032386346945228676,\n",
       "  0.027587199265447757,\n",
       "  0.02745780230034143,\n",
       "  0.027710178696239988,\n",
       "  0.02817327887363111,\n",
       "  0.02818881132795165,\n",
       "  0.027289218544804802,\n",
       "  0.034210612159222366,\n",
       "  0.032765975889439386,\n",
       "  0.02730767287624379,\n",
       "  0.028139355429448187,\n",
       "  0.029435242378773787,\n",
       "  0.02733387635477508,\n",
       "  0.027156618532414238,\n",
       "  0.027345947998886306,\n",
       "  0.029005511951011916,\n",
       "  0.0281986284457768,\n",
       "  0.027024876912279675,\n",
       "  0.02852654190501198,\n",
       "  0.030952259242379418,\n",
       "  0.030946830372946956,\n",
       "  0.02698251842909182,\n",
       "  0.02986162476396809,\n",
       "  0.02717530777445063,\n",
       "  0.02777679089922458,\n",
       "  0.02697902403306216,\n",
       "  0.02688028853541861,\n",
       "  0.026973961557572087,\n",
       "  0.0268908087319384,\n",
       "  0.028795515730356176,\n",
       "  0.026844101721265664,\n",
       "  0.02679568904762467,\n",
       "  0.027452093712054194,\n",
       "  0.027834232962535073,\n",
       "  0.027220793487504124,\n",
       "  0.030217013705987485,\n",
       "  0.027774316522603235,\n",
       "  0.028930569898026684,\n",
       "  0.026933022805800042,\n",
       "  0.03005352192558348,\n",
       "  0.027618216234259307,\n",
       "  0.026727359590586273,\n",
       "  0.026872074165536712,\n",
       "  0.02664812752433742,\n",
       "  0.026695410057436676,\n",
       "  0.02727468157730376,\n",
       "  0.026734270182593414,\n",
       "  0.026865711447317153,\n",
       "  0.026525876306307813,\n",
       "  0.027702710940502584,\n",
       "  0.029142274191447843,\n",
       "  0.027076619393968333,\n",
       "  0.02720100194370995,\n",
       "  0.026438610449743768,\n",
       "  0.03129375308441619,\n",
       "  0.029315874547076723,\n",
       "  0.026383412338327618,\n",
       "  0.027558678039349617,\n",
       "  0.026409700392590215,\n",
       "  0.026577564848897357,\n",
       "  0.030539263614142935,\n",
       "  0.026375562977045776,\n",
       "  0.028680106648243962,\n",
       "  0.026296240192217133,\n",
       "  0.028468964520531395,\n",
       "  0.02623339807226633,\n",
       "  0.02703798341099173,\n",
       "  0.02764449854924654,\n",
       "  0.029932219593320042,\n",
       "  0.026484274491667747,\n",
       "  0.02731989622892191,\n",
       "  0.026385056945340088,\n",
       "  0.02614921684920167,\n",
       "  0.02613279704237357,\n",
       "  0.02663520898980399,\n",
       "  0.028177844604942947,\n",
       "  0.029170119669288397,\n",
       "  0.026193507361070564,\n",
       "  0.026061609364114703,\n",
       "  0.029449165104112277,\n",
       "  0.02634095000879218,\n",
       "  0.027660162785711387,\n",
       "  0.026008703377253067,\n",
       "  0.02596195056491221,\n",
       "  0.027526836943191788,\n",
       "  0.025952889409381896,\n",
       "  0.03318564838846214,\n",
       "  0.025923528242856264,\n",
       "  0.026423705194611104,\n",
       "  0.030086864955956118,\n",
       "  0.028100401217428347,\n",
       "  0.026209829813645533,\n",
       "  0.0260521245150206,\n",
       "  0.02638946718070656,\n",
       "  0.025831568764988332,\n",
       "  0.02617037344413499,\n",
       "  0.027111855203596255,\n",
       "  0.025962077430449426,\n",
       "  0.03088110756749908,\n",
       "  0.027532823104411363,\n",
       "  0.025789233723965785,\n",
       "  0.026342986803501844,\n",
       "  0.03085515892598778,\n",
       "  0.025734551375110944,\n",
       "  0.02669295825374623,\n",
       "  0.02597040804879119,\n",
       "  0.026166724724074206,\n",
       "  0.03288813261509252,\n",
       "  0.026552081930761535,\n",
       "  0.02692568055354059,\n",
       "  0.025775664225996784,\n",
       "  0.02573106826360648,\n",
       "  0.0258067141364639,\n",
       "  0.026728344448686887,\n",
       "  0.026457471027970313,\n",
       "  0.027487426274456085,\n",
       "  0.02909954679198563,\n",
       "  0.027439748441490035,\n",
       "  0.02743926679249853,\n",
       "  0.025647079828195272,\n",
       "  0.027429407726352415,\n",
       "  0.025569590313049655,\n",
       "  0.025778905969733992,\n",
       "  0.026176132603238027,\n",
       "  0.029225822355753432,\n",
       "  0.025596459198277445,\n",
       "  0.027004940256786843,\n",
       "  0.025537969525127362,\n",
       "  0.02629441412476202,\n",
       "  0.02541284094331786,\n",
       "  0.02771224751874494,\n",
       "  0.02663407625320057,\n",
       "  0.025375001485614727,\n",
       "  0.02547828034342577,\n",
       "  0.02535729588319858,\n",
       "  0.027943177558093644,\n",
       "  0.025361881393473597,\n",
       "  0.025723745701058457,\n",
       "  0.028977912726501624,\n",
       "  0.025505009025800975,\n",
       "  0.02604290086310357,\n",
       "  0.027750586182810365,\n",
       "  0.02648881497249628,\n",
       "  0.026258050673641266,\n",
       "  0.025791908889853707,\n",
       "  0.025256635811335096,\n",
       "  0.028826173672374958,\n",
       "  0.025919795412725457,\n",
       "  0.025589157303329557,\n",
       "  0.02527397229569033,\n",
       "  0.025315138596730926,\n",
       "  0.02528999001563837,\n",
       "  0.02897950442469058,\n",
       "  0.02557675732532516,\n",
       "  0.025222046452108772,\n",
       "  0.02518831656780094,\n",
       "  0.025574490885871153,\n",
       "  0.025390623847488313,\n",
       "  0.02556792910133178,\n",
       "  0.034235515918893115,\n",
       "  0.025547740472635874,\n",
       "  0.025153283165612567,\n",
       "  0.025385207725533595,\n",
       "  0.02517576879278446,\n",
       "  0.025421438455426444,\n",
       "  0.026192170536766448,\n",
       "  0.02589536033725987,\n",
       "  0.025308139738626778,\n",
       "  0.025082865986041723,\n",
       "  0.025462633809850862,\n",
       "  0.02730766727666681,\n",
       "  0.025140480099556346,\n",
       "  0.029109855518133068,\n",
       "  0.025882404076401144,\n",
       "  0.02539000055597474,\n",
       "  0.025445414939895272,\n",
       "  0.02550677357163901,\n",
       "  0.027275057276710868,\n",
       "  0.02500495839631185,\n",
       "  0.025687428105932972,\n",
       "  0.027056516000690558,\n",
       "  0.02784524403590088,\n",
       "  0.027115169297515725,\n",
       "  0.026014729193411766,\n",
       "  0.026173658370195578,\n",
       "  0.02500213625608012,\n",
       "  0.02501150258273507,\n",
       "  0.02654868417303078,\n",
       "  0.025569597783032805,\n",
       "  0.02510917993883292,\n",
       "  0.02563207601197064,\n",
       "  0.027825242117978634,\n",
       "  0.027525034841770928,\n",
       "  0.024956984748132526,\n",
       "  0.026111735488908987,\n",
       "  0.024990394055688134,\n",
       "  0.025006380584090947,\n",
       "  0.024931551461728912,\n",
       "  0.025026635806231448,\n",
       "  0.025339366394716005,\n",
       "  0.026396117421487966,\n",
       "  0.025064112288722146,\n",
       "  0.024921492506594707,\n",
       "  0.02529022442176938,\n",
       "  0.02640482858599474,\n",
       "  0.025270928954705597,\n",
       "  0.024881822099753967,\n",
       "  0.02691886028042063,\n",
       "  0.026467931701336055,\n",
       "  0.024839938203028093,\n",
       "  0.024871684017125518,\n",
       "  0.026053622380519906,\n",
       "  0.02481815068749711,\n",
       "  0.02506582809534545,\n",
       "  0.033930475862386324,\n",
       "  0.025359673029743134,\n",
       "  0.024796663729163507,\n",
       "  0.02479271376117443,\n",
       "  0.027886993381738043,\n",
       "  0.025101844869398822,\n",
       "  0.025480211685256413,\n",
       "  0.02481869199934105,\n",
       "  0.025097519687066475,\n",
       "  0.024822731933090834,\n",
       "  0.024791695670379945,\n",
       "  0.0257545634599713,\n",
       "  0.024766224331688135,\n",
       "  0.025699480743302652,\n",
       "  0.02987692038101765,\n",
       "  0.024847884045448153,\n",
       "  0.027239363462043305,\n",
       "  0.02471043139618511,\n",
       "  0.025241762166842818,\n",
       "  0.024663037562277168,\n",
       "  0.02619114290573634,\n",
       "  0.025616964608586083,\n",
       "  0.025059884487806508,\n",
       "  0.026170252170413733,\n",
       "  0.029566662426805123,\n",
       "  0.028416389741081124,\n",
       "  0.024679409720314047,\n",
       "  0.024661692907102406,\n",
       "  0.02657713880762458,\n",
       "  0.024806376562143365,\n",
       "  0.02505992429020504,\n",
       "  0.02458977004280314,\n",
       "  0.024958443618379532,\n",
       "  0.024641870408474157,\n",
       "  0.025154774678715814,\n",
       "  0.024608700586638103,\n",
       "  0.027513356429214278,\n",
       "  0.02516894560540095,\n",
       "  0.024994263289651524,\n",
       "  0.024553489262082926,\n",
       "  0.024590328386208664,\n",
       "  0.024545832921285184,\n",
       "  0.02585425108166722,\n",
       "  0.029485173360444605,\n",
       "  0.024524676729924977,\n",
       "  0.025008826275976996,\n",
       "  0.026205533494551975,\n",
       "  0.024641437576307604,\n",
       "  0.024544021235002825,\n",
       "  0.025561686938938996,\n",
       "  0.02807771417622765,\n",
       "  0.024677656346466393,\n",
       "  0.025015135284047574,\n",
       "  0.024756069861662885,\n",
       "  0.02459078251073758,\n",
       "  0.024781394357948253,\n",
       "  0.02942577889771201,\n",
       "  0.029907768809547028,\n",
       "  0.02784914115133385,\n",
       "  0.02467404621420428,\n",
       "  0.02471718650776893,\n",
       "  0.025146705905596416,\n",
       "  0.024958584015257655,\n",
       "  0.02494657242204994,\n",
       "  0.024699254454268762,\n",
       "  0.02557751810721432,\n",
       "  0.02452625894996648,\n",
       "  0.024476746939277896,\n",
       "  0.024669961433392017,\n",
       "  0.026226910192053764,\n",
       "  0.02678503480468256,\n",
       "  0.02579574426054023,\n",
       "  0.024629444962677858,\n",
       "  0.03016281931777485,\n",
       "  0.025134495350842674,\n",
       "  0.024633527523837982,\n",
       "  0.024696880062886825,\n",
       "  0.0245302254644533,\n",
       "  0.027966648635144035,\n",
       "  0.027258718789865573,\n",
       "  0.024689746007788928,\n",
       "  0.024510958790779115,\n",
       "  0.02742406092584133,\n",
       "  0.026148382974012445,\n",
       "  0.02451418838851775,\n",
       "  0.024764843149265896,\n",
       "  0.02445225795575728,\n",
       "  0.024456116269963482,\n",
       "  0.024636423483025284,\n",
       "  0.025382833374897017,\n",
       "  0.024545601677770416,\n",
       "  0.027727163856616242,\n",
       "  0.025187858005908007,\n",
       "  0.024410226300824435,\n",
       "  0.029221746075199916,\n",
       "  0.024482115916907788,\n",
       "  0.02447926178574562,\n",
       "  0.02462987780260543,\n",
       "  0.024558460588256517,\n",
       "  0.025790823575031634,\n",
       "  0.025395719080309694,\n",
       "  0.024389759660698472,\n",
       "  0.024406431696843357,\n",
       "  0.024433990630010765,\n",
       "  0.0275170153627793,\n",
       "  0.024714131362270565,\n",
       "  0.024702955464211602,\n",
       "  0.024549753319782517,\n",
       "  0.026345185617295406,\n",
       "  0.024412288838842264,\n",
       "  0.02459327142763262,\n",
       "  0.024503499067698917,\n",
       "  0.024378146700716266,\n",
       "  0.024442867290539048,\n",
       "  0.026489051912600797,\n",
       "  0.02446474290530508,\n",
       "  0.02479763620455439,\n",
       "  0.02484445561810086,\n",
       "  0.028484553665233156,\n",
       "  0.02545943894268324,\n",
       "  0.025045371478578698,\n",
       "  0.02613931861706078,\n",
       "  0.024580575840082018,\n",
       "  0.03026499668097434,\n",
       "  0.02437493877951056,\n",
       "  0.025026642946371188,\n",
       "  0.02484439020433153,\n",
       "  0.027371637369894113,\n",
       "  0.024406838719733058,\n",
       "  0.026935267844237387,\n",
       "  0.024410659373582652,\n",
       "  0.024361270972682782,\n",
       "  0.024364651980188987,\n",
       "  0.024402697516294817,\n",
       "  0.02454978252062574,\n",
       "  0.025321852867879593,\n",
       "  0.02435731088820224,\n",
       "  0.025434709627491732,\n",
       "  0.029032802830139797,\n",
       "  0.026768652231354887,\n",
       "  0.02807081356877461,\n",
       "  0.02550412346414911,\n",
       "  0.02433837961871177,\n",
       "  0.024369218623420844,\n",
       "  0.028022679725351433,\n",
       "  0.025244984215047832,\n",
       "  0.025177908265807975,\n",
       "  0.027807621269797286,\n",
       "  0.026876867290896674,\n",
       "  0.02828939400302867,\n",
       "  0.02630051575639906,\n",
       "  0.02435986427590251,\n",
       "  0.024333111313171684,\n",
       "  0.024276857473887505,\n",
       "  0.024324455903843044,\n",
       "  0.025131813743306943,\n",
       "  0.02431945438729599,\n",
       "  0.02491887523404633,\n",
       "  0.024792009875333556,\n",
       "  0.02475424101188158,\n",
       "  0.025231458139993872,\n",
       "  0.025065806017179663,\n",
       "  0.027280653200189894,\n",
       "  0.02486177146201953,\n",
       "  0.030355498908708492,\n",
       "  0.024623775963361065,\n",
       "  0.026411319518228994,\n",
       "  0.025199833477381618,\n",
       "  0.024176717623292158,\n",
       "  0.024660660192603247,\n",
       "  0.024280702117054413,\n",
       "  0.024510877063342682,\n",
       "  0.02433376112409557,\n",
       "  0.024579473487877597,\n",
       "  0.02497773210052401,\n",
       "  0.02436822422702486,\n",
       "  0.02417031443134571,\n",
       "  0.024669928047417973,\n",
       "  0.03238868061453104,\n",
       "  0.02430364926888918,\n",
       "  0.0241601041246516,\n",
       "  0.02444198605371639,\n",
       "  0.026187720176919053,\n",
       "  0.024691481589494895,\n",
       "  0.024939159866577635,\n",
       "  0.02688670446902203,\n",
       "  0.028171439313640197,\n",
       "  0.024137917428743095,\n",
       "  0.024194170181484273,\n",
       "  0.024249977754273764,\n",
       "  0.034123069084792707,\n",
       "  0.024879587827793634,\n",
       "  0.02518633269937709,\n",
       "  0.024251620327898613,\n",
       "  0.025582785840379076,\n",
       "  0.024133978167083115,\n",
       "  0.025166902396207055,\n",
       "  0.0244475833994026,\n",
       "  0.02449294364002223,\n",
       "  0.02477581285056658,\n",
       "  0.025298489475001892,\n",
       "  0.024401259166188537,\n",
       "  0.025711836064389596,\n",
       "  0.02425844417496895,\n",
       "  0.024924551978862532,\n",
       "  0.024466085785146183,\n",
       "  0.025706926574154448,\n",
       "  0.024760483471133434,\n",
       "  0.024260831449646504],\n",
       " 'val_accuracy': [1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.96666664,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0]}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "History_iris.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KWR3hsEE_ZGS"
   },
   "source": [
    "> ## 5) 학습 결과 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1240,
     "status": "ok",
     "timestamp": 1574489682383,
     "user": {
      "displayName": "이정구",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCJOG97rou_jpwsvziQA7tTe2JnwoyotzfHjlO2=s64",
      "userId": "17400242557229223804"
     },
     "user_tz": -540
    },
    "id": "OMpwabfNq2_v",
    "outputId": "a77fc39e-def1-4389-fa7b-84b8bce430a9"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'acc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-1764fd2adcf0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHistory_iris\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHistory_iris\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHistory_iris\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHistory_iris\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'acc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'val_acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'acc'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize = (9, 6))\n",
    "plt.plot(History_iris.history['loss'])\n",
    "plt.plot(History_iris.history['val_loss'])\n",
    "plt.plot(History_iris.history['acc'])\n",
    "plt.plot(History_iris.history['val_acc'])\n",
    "plt.legend(['loss', 'val_loss', 'acc', 'val_acc'])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FNq1blJf_eIW"
   },
   "source": [
    "> ## 6) Model Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dok6iL5RKeTP"
   },
   "source": [
    "* Loss & Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 742,
     "status": "ok",
     "timestamp": 1574489913567,
     "user": {
      "displayName": "이정구",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCJOG97rou_jpwsvziQA7tTe2JnwoyotzfHjlO2=s64",
      "userId": "17400242557229223804"
     },
     "user_tz": -540
    },
    "id": "fJ2HtnYYsPIE",
    "outputId": "fd5cc8e7-8e82-4ac6-c62b-df935569768f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "30/30 [==============================] - 0s 76us/step\n",
      "Loss = 0.04\n",
      "Accuracy = 0.97\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = Model_iris.evaluate(x_test, y_test)\n",
    "\n",
    "print('Loss = {:.2f}'.format(loss))\n",
    "print('Accuracy = {:.2f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jFjd3_I7_mqn"
   },
   "source": [
    "> ## 7) Model Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CGP07vr3_rAK"
   },
   "source": [
    "* Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "St1qcFxQuKqB"
   },
   "outputs": [],
   "source": [
    "Model_iris.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M1Ph1WOS_uCA"
   },
   "source": [
    "* Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 686,
     "status": "ok",
     "timestamp": 1574490007215,
     "user": {
      "displayName": "이정구",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCJOG97rou_jpwsvziQA7tTe2JnwoyotzfHjlO2=s64",
      "userId": "17400242557229223804"
     },
     "user_tz": -540
    },
    "id": "A3GwUpZfs1BT",
    "outputId": "84a7a014-60e4-4481-8237-31cd0bf2a69e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 2, 0, 1, 2, 0, 2, 0, 2, 2, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,\n",
       "       2, 0, 2, 2, 0, 1, 1, 2])"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = Model_iris.predict_classes(x_test)\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "spr7DJmE_62e"
   },
   "source": [
    "* One Hot Encoding to Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pcI9Od5ftye5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1192,
     "status": "ok",
     "timestamp": 1574490112263,
     "user": {
      "displayName": "이정구",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCJOG97rou_jpwsvziQA7tTe2JnwoyotzfHjlO2=s64",
      "userId": "17400242557229223804"
     },
     "user_tz": -540
    },
    "id": "8xv4SLZm0nzn",
    "outputId": "ea89514d-8f9e-4d3a-b5ce-243130d43f01"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 2, 0, 1, 2, 0, 2, 0, 1, 2, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,\n",
       "       2, 0, 2, 2, 0, 1, 1, 2])"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.argmax(y_test, axis = 1)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y-hzFPiDAEyo"
   },
   "source": [
    "* Confusion Matrix & Claasification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1059,
     "status": "ok",
     "timestamp": 1574490115055,
     "user": {
      "displayName": "이정구",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCJOG97rou_jpwsvziQA7tTe2JnwoyotzfHjlO2=s64",
      "userId": "17400242557229223804"
     },
     "user_tz": -540
    },
    "id": "ECnlNwrRzZve",
    "outputId": "bef45b6b-ea03-4c3d-a627-7e8d6cc8bb48"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[13,  0,  0],\n",
       "       [ 0,  8,  1],\n",
       "       [ 0,  0,  8]])"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 688,
     "status": "ok",
     "timestamp": 1574490117132,
     "user": {
      "displayName": "이정구",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCJOG97rou_jpwsvziQA7tTe2JnwoyotzfHjlO2=s64",
      "userId": "17400242557229223804"
     },
     "user_tz": -540
    },
    "id": "6xW7Tw8Luk8O",
    "outputId": "c96f2c6a-7937-46f8-b708-cd43a26506f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        13\n",
      "   virginica       1.00      0.89      0.94         9\n",
      "  versicolor       0.89      1.00      0.94         8\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.96      0.96      0.96        30\n",
      "weighted avg       0.97      0.97      0.97        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y, y_hat, \n",
    "                            target_names = ['setosa',\n",
    "                                            'virginica',\n",
    "                                            'versicolor']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N6YjXuM2AeEA"
   },
   "source": [
    "# III. Model Save & Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QJHanSlYBMaD"
   },
   "source": [
    "> ## 1) File System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9fukCVfZBreP"
   },
   "source": [
    "* Save to Colab File System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3056,
     "status": "ok",
     "timestamp": 1574490365973,
     "user": {
      "displayName": "이정구",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCJOG97rou_jpwsvziQA7tTe2JnwoyotzfHjlO2=s64",
      "userId": "17400242557229223804"
     },
     "user_tz": -540
    },
    "id": "FXz2D-PxAn0p",
    "outputId": "d7eb613e-6589-4171-f029-a2bf85c87914"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iris.csv  Model_iris.h5  sample_data\n"
     ]
    }
   ],
   "source": [
    "Model_iris.save('Model_iris.h5')\n",
    "\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mnABg40YB0Ex"
   },
   "source": [
    "* Download Colab File System to Local File System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UROGkgi7B3J6"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "files.download('Model_iris.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "50eGEKhzDbVg"
   },
   "source": [
    "* Load from Colab File System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SlNcgVb9BZQz"
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "Model_iris = load_model('Model_iris.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q24HQDn3Bbmv"
   },
   "source": [
    "> ## 2) Google Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-DoXDaDQD_nR"
   },
   "source": [
    "* Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XUcAHm-oEThi"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R3yaKi3GEftH"
   },
   "source": [
    "* Check Mounted_Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2563,
     "status": "ok",
     "timestamp": 1574490581850,
     "user": {
      "displayName": "이정구",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCJOG97rou_jpwsvziQA7tTe2JnwoyotzfHjlO2=s64",
      "userId": "17400242557229223804"
     },
     "user_tz": -540
    },
    "id": "Y96H0gSfEl0-",
    "outputId": "6f35d511-523c-4161-faf6-3e00995de9b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cats_and_dogs_small  jena_climate  PII.csv\n"
     ]
    }
   ],
   "source": [
    "!ls '/content/drive/My Drive/Colab Notebooks/datasets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1115,
     "status": "ok",
     "timestamp": 1574490588127,
     "user": {
      "displayName": "이정구",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCJOG97rou_jpwsvziQA7tTe2JnwoyotzfHjlO2=s64",
      "userId": "17400242557229223804"
     },
     "user_tz": -540
    },
    "id": "zVPr1YPKE2mk",
    "outputId": "dd122b93-d057-475a-d07e-bbd4e8613f92"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Grade</th>\n",
       "      <th>Picture</th>\n",
       "      <th>BloodType</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>송태섭</td>\n",
       "      <td>남자</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>무</td>\n",
       "      <td>B</td>\n",
       "      <td>179.1</td>\n",
       "      <td>63.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>최유정</td>\n",
       "      <td>여자</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>유</td>\n",
       "      <td>A</td>\n",
       "      <td>177.1</td>\n",
       "      <td>54.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>이한나</td>\n",
       "      <td>여자</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>무</td>\n",
       "      <td>A</td>\n",
       "      <td>167.9</td>\n",
       "      <td>50.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Name Gender  Age  Grade Picture BloodType  Height  Weight\n",
       "0  송태섭     남자   21      3       무         B   179.1    63.9\n",
       "1  최유정     여자   23      1       유         A   177.1    54.9\n",
       "2  이한나     여자   20      1       무         A   167.9    50.2"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "DF = pd.read_csv('/content/drive/My Drive/Colab Notebooks/datasets/PII.csv')\n",
    "DF.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "izCHadxzFuj-"
   },
   "source": [
    "* Save to Mounted Google Drive Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VYxWBnRXF1sm"
   },
   "outputs": [],
   "source": [
    "Model_iris.save('/content/drive/My Drive/Colab Notebooks/models/Model_iris.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2503,
     "status": "ok",
     "timestamp": 1574490786628,
     "user": {
      "displayName": "이정구",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCJOG97rou_jpwsvziQA7tTe2JnwoyotzfHjlO2=s64",
      "userId": "17400242557229223804"
     },
     "user_tz": -540
    },
    "id": "62Ks4RIcGM5e",
    "outputId": "aaf76abd-e2b7-4d5d-ffc4-66106c151137"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dogs_and_cats_augmentation.h5  dogs_and_cats_small.h5  yena_lstm.h5\n",
      "dogs_and_cats_feature.h5       Model_iris.h5\t       yena_simpleRNN.h5\n",
      "dogs_and_cats_fineTuning.h5    yena_lstm_dropout.h5    yena_stacked_GRU.h5\n",
      "dogs_and_cats_pretrained.h5    yena_lstm_GPU.h5\n"
     ]
    }
   ],
   "source": [
    "!ls '/content/drive/My Drive/Colab Notebooks/models'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9qHYg5fPGnaB"
   },
   "source": [
    "* Load from Mounted Google Drive Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-c2nlwDbGcVZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "Model_iris = load_model('/content/drive/My Drive/Colab Notebooks/models/Model_iris.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "prgTZL2h1A2Z"
   },
   "source": [
    "# \n",
    "# \n",
    "# \n",
    "# The End\n",
    "# \n",
    "# \n",
    "# "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "000_DNN_iris_Modeling (1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
